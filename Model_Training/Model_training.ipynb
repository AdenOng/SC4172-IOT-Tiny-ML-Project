{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def load_imu_segments(file_path):\n",
    "    \"\"\"\n",
    "    Loads labeled IMU segments from a pickle file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the pickle file.\n",
    "\n",
    "    Returns:\n",
    "        object: The object loaded from the pickle file.  This will\n",
    "                likely be a list, dictionary, or custom object\n",
    "                depending on how the pickle file was created.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:  # 'rb' for read binary\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'labeled_imu_segments.pkl'\n",
    "labeled_segments = load_imu_segments(file_path)\n",
    "\n",
    "if labeled_segments:\n",
    "    print(\"Data loaded successfully.\")\n",
    "    # Now you can work with the loaded_data object.\n",
    "    # For example, if it's a list:\n",
    "    # for item in loaded_data:\n",
    "    #     print(item)\n",
    "else:\n",
    "    print(\"Failed to load data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Keras model from imu_cnn_model.h5...\n",
      "Model loaded successfully.\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_12 (Conv1D)          (None, 45, 8)             368       \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 45, 8)             0         \n",
      "                                                                 \n",
      " conv1d_13 (Conv1D)          (None, 45, 16)            656       \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 45, 16)            0         \n",
      "                                                                 \n",
      " global_average_pooling1d_6  (None, 16)                0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4)                 68        \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Total params: 1092 (4.27 KB)\n",
      "Trainable params: 1092 (4.27 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Scaler file found at imu_scaler.pkl. Ensure you use its values on the MCU.\n",
      "Converting model to TensorFlow Lite...\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp15xa0dqg/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp15xa0dqg/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted successfully.\n",
      "TFLite model saved to: imu_model.tflite (8392 bytes)\n",
      "\n",
      "Converting TFLite model to C source file using xxd...\n",
      "C array saved to: imu_model.h\n",
      "You can now include this .h file in your Arduino project.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 10:52:27.999751: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-04-03 10:52:27.999765: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-04-03 10:52:27.999856: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp15xa0dqg\n",
      "2025-04-03 10:52:28.000419: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-04-03 10:52:28.000425: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmp15xa0dqg\n",
      "2025-04-03 10:52:28.002348: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-04-03 10:52:28.018912: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmp15xa0dqg\n",
      "2025-04-03 10:52:28.023984: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 24128 microseconds.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "H5_MODEL_PATH = 'imu_cnn_model.h5'        # Input Keras model saved in HDF5 format\n",
    "TFLITE_MODEL_PATH = 'imu_model.tflite'    # Output TFLite model path\n",
    "TFLITE_MODEL_CC_PATH = 'imu_model.cc'   # Output TFLite C array path (optional)\n",
    "SCALER_PATH = 'imu_scaler.pkl'          # Path to the saved scaler\n",
    "\n",
    "# --- Load the Trained Keras Model ---\n",
    "if not os.path.exists(H5_MODEL_PATH):\n",
    "    print(f\"Error: Model file not found at {H5_MODEL_PATH}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Loading Keras model from {H5_MODEL_PATH}...\")\n",
    "model = tf.keras.models.load_model(H5_MODEL_PATH)\n",
    "print(\"Model loaded successfully.\")\n",
    "model.summary() # Print model summary to verify\n",
    "\n",
    "# --- Load the Scaler (Optional here, but good practice to verify it exists) ---\n",
    "if not os.path.exists(SCALER_PATH):\n",
    "    print(f\"Warning: Scaler file not found at {SCALER_PATH}. Conversion will proceed, but ensure you have the mean/scale values.\")\n",
    "else:\n",
    "    print(f\"Scaler file found at {SCALER_PATH}. Ensure you use its values on the MCU.\")\n",
    "    # Optionally load and print mean/scale here for verification if needed\n",
    "    # with open(SCALER_PATH, 'rb') as f:\n",
    "    #     scaler = pickle.load(f)\n",
    "    # print(f\"Scaler Means: {scaler.mean_}\")\n",
    "    # print(f\"Scaler Scales: {scaler.scale_}\")\n",
    "\n",
    "\n",
    "# --- Convert to TensorFlow Lite ---\n",
    "print(\"Converting model to TensorFlow Lite...\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# Apply default optimizations (recommended for MCUs, often includes float16 quantization)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# --- Optional: Integer Quantization (More complex, requires representative dataset) ---\n",
    "# If DEFAULT optimizations are not small enough or you need integer math:\n",
    "# 1. Load your SCALED training data (X_train_scaled from the training script)\n",
    "# 2. Define a representative dataset generator:\n",
    "# def representative_dataset_gen():\n",
    "#     # Yield a small number (e.g., 100) of samples from SCALED training data\n",
    "#     # Ensure data is float32\n",
    "#     for i in range(100):\n",
    "#         yield [X_train_scaled[i:i+1].astype(np.float32)] # Must be a list\n",
    "#\n",
    "# converter.representative_dataset = representative_dataset_gen\n",
    "# # Force input and output tensors to int8 (common for MCUs)\n",
    "# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "# converter.inference_output_type = tf.int8 # or tf.uint8\n",
    "# print(\"INFO: Using Integer Quantization settings.\")\n",
    "# --- End Optional Integer Quantization ---\n",
    "\n",
    "\n",
    "# Perform the conversion\n",
    "try:\n",
    "    tflite_model = converter.convert()\n",
    "    print(\"Model converted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n!!!!!!!! ERROR during TFLite conversion !!!!!!!!!\")\n",
    "    print(f\"Error message: {e}\")\n",
    "    print(\"This can happen due to unsupported operations or issues during quantization.\")\n",
    "    print(\"Try without optimizations first, or ensure your representative dataset (if used) is correct.\")\n",
    "    print(\"Check TensorFlow documentation for compatibility.\")\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    exit() # Stop if conversion fails\n",
    "\n",
    "\n",
    "# --- Save the TFLite Model ---\n",
    "with open(TFLITE_MODEL_PATH, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "print(f\"TFLite model saved to: {TFLITE_MODEL_PATH} ({len(tflite_model)} bytes)\")\n",
    "\n",
    "\n",
    "# --- Optional: Convert TFLite model to C array using xxd ---\n",
    "# This is often the easiest way to include the model in Arduino projects\n",
    "print(\"\\nConverting TFLite model to C source file using xxd...\")\n",
    "# Make sure xxd is available in your system PATH (common on Linux/macOS, installable on Windows)\n",
    "try:\n",
    "    # Generate C array file (e.g., imu_model.cc)\n",
    "    os.system(f\"xxd -i {TFLITE_MODEL_PATH} > {TFLITE_MODEL_CC_PATH}\")\n",
    "\n",
    "    # Optional: Read the C file and modify variable name for clarity\n",
    "    with open(TFLITE_MODEL_CC_PATH, 'r') as f:\n",
    "        c_content = f.read()\n",
    "    # Replace the default variable name generated by xxd\n",
    "    # (adjust 'imu_model_tflite' if your filename is different)\n",
    "    c_content = c_content.replace('unsigned char imu_model_tflite[] = {',\n",
    "                                  'const unsigned char g_imu_model_data[] = {')\n",
    "    c_content = c_content.replace('unsigned int imu_model_tflite_len = ',\n",
    "                                  'const unsigned int g_imu_model_data_len = ')\n",
    "    # Add header guard and include\n",
    "    c_output = f\"#ifndef IMU_MODEL_DATA_H_\\n#define IMU_MODEL_DATA_H_\\n\\n{c_content}\\n#endif // IMU_MODEL_DATA_H_\\n\"\n",
    "\n",
    "    with open(TFLITE_MODEL_CC_PATH.replace('.cc', '.h'), 'w') as f: # Save as .h\n",
    "         f.write(c_output)\n",
    "\n",
    "    print(f\"C array saved to: {TFLITE_MODEL_CC_PATH.replace('.cc', '.h')}\")\n",
    "    print(\"You can now include this .h file in your Arduino project.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nError: 'xxd' command not found.\")\n",
    "    print(\"Please install 'xxd' (part of vim-common or similar packages)\")\n",
    "    print(f\"or manually convert '{TFLITE_MODEL_PATH}' to a C array.\")\n",
    "except Exception as e:\n",
    "     print(f\"\\nError during xxd conversion or C file modification: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.85      0.88        39\n",
      "           1       0.77      0.77      0.77        57\n",
      "           2       0.87      0.87      0.87        62\n",
      "           3       0.80      0.84      0.82        51\n",
      "\n",
      "    accuracy                           0.83       209\n",
      "   macro avg       0.84      0.83      0.84       209\n",
      "weighted avg       0.83      0.83      0.83       209\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load your test data (X_test_scaled and y_test should be pre-loaded and preprocessed as before)\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"imu_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Prepare a list to collect predictions.\n",
    "predictions = []\n",
    "\n",
    "# Loop over each sample in your test set.\n",
    "for i in range(len(X_test_scaled)):\n",
    "    # Ensure the sample is of the expected shape and type.\n",
    "    sample = np.expand_dims(X_test_scaled[i], axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], sample)\n",
    "    \n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Retrieve the output.\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    predictions.append(output_data)\n",
    "\n",
    "# Convert predictions list to an array.\n",
    "predictions = np.vstack(predictions)\n",
    "\n",
    "# Convert probabilities to predicted class indices.\n",
    "y_pred_classes = np.argmax(predictions, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report.\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_classes, y_pred_classes))\n",
    "\n",
    "# Compute and display confusion matrix.\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=np.unique(y_true_classes),\n",
    "            yticklabels=np.unique(y_true_classes))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Keras model from imu_cnn_model.h5...\n",
      "Model loaded successfully.\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_1 (Conv1D)           (None, 1, 45, 8)          368       \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 1, 45, 8)          0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 1, 45, 16)         656       \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 1, 45, 16)         0         \n",
      "                                                                 \n",
      " global_avg_pool (GlobalAve  (None, 16)                0         \n",
      " ragePooling2D)                                                  \n",
      "                                                                 \n",
      " output_dense (Dense)        (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1092 (4.27 KB)\n",
      "Trainable params: 1092 (4.27 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Scaler file found at imu_scaler.pkl. Ensure you use its values on the MCU.\n",
      "Converting model to TensorFlow Lite...\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpohd_biiv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpohd_biiv/assets\n",
      "2025-04-03 13:35:32.675133: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-04-03 13:35:32.675147: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted successfully.\n",
      "TFLite model saved to: imu_model.tflite (13292 bytes)\n",
      "\n",
      "Converting TFLite model to C source file using xxd...\n",
      "C array saved to: imu_model.h\n",
      "You can now include this .h file in your Arduino project.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 13:35:32.675239: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpohd_biiv\n",
      "2025-04-03 13:35:32.676011: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-04-03 13:35:32.676017: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpohd_biiv\n",
      "2025-04-03 13:35:32.678370: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-04-03 13:35:32.698102: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpohd_biiv\n",
      "2025-04-03 13:35:32.704389: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 29149 microseconds.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "H5_MODEL_PATH = 'imu_cnn_model.h5'        # Input Keras model saved in HDF5 format\n",
    "TFLITE_MODEL_PATH = 'imu_model.tflite'    # Output TFLite model path\n",
    "TFLITE_MODEL_CC_PATH = 'imu_model.cc'   # Output TFLite C array path (optional)\n",
    "SCALER_PATH = 'imu_scaler.pkl'          # Path to the saved scaler\n",
    "\n",
    "# --- Load the Trained Keras Model ---\n",
    "if not os.path.exists(H5_MODEL_PATH):\n",
    "    print(f\"Error: Model file not found at {H5_MODEL_PATH}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Loading Keras model from {H5_MODEL_PATH}...\")\n",
    "model = tf.keras.models.load_model(H5_MODEL_PATH)\n",
    "print(\"Model loaded successfully.\")\n",
    "model.summary() # Print model summary to verify\n",
    "\n",
    "# --- Load the Scaler (Optional here, but good practice to verify it exists) ---\n",
    "if not os.path.exists(SCALER_PATH):\n",
    "    print(f\"Warning: Scaler file not found at {SCALER_PATH}. Conversion will proceed, but ensure you have the mean/scale values.\")\n",
    "else:\n",
    "    print(f\"Scaler file found at {SCALER_PATH}. Ensure you use its values on the MCU.\")\n",
    "    # Optionally load and print mean/scale here for verification if needed\n",
    "    # with open(SCALER_PATH, 'rb') as f:\n",
    "    #     scaler = pickle.load(f)\n",
    "    # print(f\"Scaler Means: {scaler.mean_}\")\n",
    "    # print(f\"Scaler Scales: {scaler.scale_}\")\n",
    "\n",
    "\n",
    "# --- Convert to TensorFlow Lite ---\n",
    "print(\"Converting model to TensorFlow Lite...\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# Apply default optimizations (recommended for MCUs, often includes float16 quantization)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# --- Optional: Integer Quantization (More complex, requires representative dataset) ---\n",
    "# If DEFAULT optimizations are not small enough or you need integer math:\n",
    "# 1. Load your SCALED training data (X_train_scaled from the training script)\n",
    "# 2. Define a representative dataset generator:\n",
    "# def representative_dataset_gen():\n",
    "#     # Yield a small number (e.g., 100) of samples from SCALED training data\n",
    "#     # Ensure data is float32\n",
    "#     for i in range(100):\n",
    "#         yield [X_train_scaled[i:i+1].astype(np.float32)] # Must be a list\n",
    "#\n",
    "# converter.representative_dataset = representative_dataset_gen\n",
    "# # Force input and output tensors to int8 (common for MCUs)\n",
    "# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "# converter.inference_output_type = tf.int8 # or tf.uint8\n",
    "# print(\"INFO: Using Integer Quantization settings.\")\n",
    "# --- End Optional Integer Quantization ---\n",
    "\n",
    "\n",
    "# Perform the conversion\n",
    "try:\n",
    "    tflite_model = converter.convert()\n",
    "    print(\"Model converted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n!!!!!!!! ERROR during TFLite conversion !!!!!!!!!\")\n",
    "    print(f\"Error message: {e}\")\n",
    "    print(\"This can happen due to unsupported operations or issues during quantization.\")\n",
    "    print(\"Try without optimizations first, or ensure your representative dataset (if used) is correct.\")\n",
    "    print(\"Check TensorFlow documentation for compatibility.\")\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    exit() # Stop if conversion fails\n",
    "\n",
    "\n",
    "# --- Save the TFLite Model ---\n",
    "with open(TFLITE_MODEL_PATH, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "print(f\"TFLite model saved to: {TFLITE_MODEL_PATH} ({len(tflite_model)} bytes)\")\n",
    "\n",
    "\n",
    "# --- Optional: Convert TFLite model to C array using xxd ---\n",
    "# This is often the easiest way to include the model in Arduino projects\n",
    "print(\"\\nConverting TFLite model to C source file using xxd...\")\n",
    "# Make sure xxd is available in your system PATH (common on Linux/macOS, installable on Windows)\n",
    "try:\n",
    "    # Generate C array file (e.g., imu_model.cc)\n",
    "    os.system(f\"xxd -i {TFLITE_MODEL_PATH} > {TFLITE_MODEL_CC_PATH}\")\n",
    "\n",
    "    # Optional: Read the C file and modify variable name for clarity\n",
    "    with open(TFLITE_MODEL_CC_PATH, 'r') as f:\n",
    "        c_content = f.read()\n",
    "    # Replace the default variable name generated by xxd\n",
    "    # (adjust 'imu_model_tflite' if your filename is different)\n",
    "    c_content = c_content.replace('unsigned char imu_model_tflite[] = {',\n",
    "                                  'const unsigned char g_imu_model_data[] = {')\n",
    "    c_content = c_content.replace('unsigned int imu_model_tflite_len = ',\n",
    "                                  'const unsigned int g_imu_model_data_len = ')\n",
    "    # Add header guard and include\n",
    "    c_output = f\"#ifndef IMU_MODEL_DATA_H_\\n#define IMU_MODEL_DATA_H_\\n\\n{c_content}\\n#endif // IMU_MODEL_DATA_H_\\n\"\n",
    "\n",
    "    with open(TFLITE_MODEL_CC_PATH.replace('.cc', '.h'), 'w') as f: # Save as .h\n",
    "         f.write(c_output)\n",
    "\n",
    "    print(f\"C array saved to: {TFLITE_MODEL_CC_PATH.replace('.cc', '.h')}\")\n",
    "    print(\"You can now include this .h file in your Arduino project.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nError: 'xxd' command not found.\")\n",
    "    print(\"Please install 'xxd' (part of vim-common or similar packages)\")\n",
    "    print(f\"or manually convert '{TFLITE_MODEL_PATH}' to a C array.\")\n",
    "except Exception as e:\n",
    "     print(f\"\\nError during xxd conversion or C file modification: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.13.1\n",
      "!!!!!!!!!!!!!!!!!\n",
      "!!! No GPU found using `tf.config.list_physical_devices('GPU')`.\n",
      "!!! Training will proceed on CPU.\n",
      "!!! Check TensorFlow installation, NVIDIA drivers, CUDA, and cuDNN.\n",
      "!!!!!!!!!!!!!!!!!\n",
      "Successfully loaded data from 'labeled_imu_segments.pkl' (1390 segments)\n",
      "Detected segment shape: (45, 9)\n",
      "Labels encoded.\n",
      "Class mapping: {0: 'jump', 1: 'null', 2: 'spin', 3: 'weave'}\n",
      "Target model input shape: (1, 45, 9)\n",
      "Data split (before reshape): Train=(903, 45, 9), Validation=(278, 45, 9), Test=(209, 45, 9)\n",
      "Data scaled using StandardScaler (fitted on training data).\n",
      "Reshaping data to add dimension for TFLM Conv2D compatibility...\n",
      "Data shapes after reshape: Train=(903, 1, 45, 9), Validation=(278, 1, 45, 9), Test=(209, 1, 45, 9)\n",
      "Calculated Class Weights: {0: 1.3279411764705882, 1: 0.9214285714285714, 2: 0.8392193308550185, 3: 1.0308219178082192}\n",
      "Building model with Input shape: (1, 45, 9)\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_1 (Conv2D)           (None, 1, 45, 8)          368       \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 1, 45, 8)          0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 1, 45, 16)         656       \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 1, 45, 16)         0         \n",
      "                                                                 \n",
      " global_avg_pool (GlobalAve  (None, 16)                0         \n",
      " ragePooling2D)                                                  \n",
      "                                                                 \n",
      " output_dense (Dense)        (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1092 (4.27 KB)\n",
      "Trainable params: 1092 (4.27 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n",
      "Epoch 1/100\n",
      " 1/57 [..............................] - ETA: 12s - loss: 1.4176 - accuracy: 0.1875\n",
      "Epoch 1: val_loss improved from inf to 1.28612, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 1.3519 - accuracy: 0.2414 - val_loss: 1.2861 - val_accuracy: 0.3489\n",
      "Epoch 2/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 1.3094 - accuracy: 0.1875\n",
      "Epoch 2: val_loss improved from 1.28612 to 1.14678, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 1.2452 - accuracy: 0.3920 - val_loss: 1.1468 - val_accuracy: 0.5647\n",
      "Epoch 3/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 1.1196 - accuracy: 0.6250\n",
      "Epoch 3: val_loss improved from 1.14678 to 1.01797, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 1.1392 - accuracy: 0.5604 - val_loss: 1.0180 - val_accuracy: 0.6187\n",
      "Epoch 4/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 1.2010 - accuracy: 0.5625\n",
      "Epoch 4: val_loss improved from 1.01797 to 0.94062, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 1.0578 - accuracy: 0.5891 - val_loss: 0.9406 - val_accuracy: 0.6763\n",
      "Epoch 5/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.8647 - accuracy: 0.6250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aden/anaconda3/envs/TFenv/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: val_loss improved from 0.94062 to 0.90207, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.9959 - accuracy: 0.6168 - val_loss: 0.9021 - val_accuracy: 0.6727\n",
      "Epoch 6/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.8924 - accuracy: 0.6875\n",
      "Epoch 6: val_loss improved from 0.90207 to 0.87330, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.9499 - accuracy: 0.6689 - val_loss: 0.8733 - val_accuracy: 0.6871\n",
      "Epoch 7/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 1.1125 - accuracy: 0.5000\n",
      "Epoch 7: val_loss improved from 0.87330 to 0.81669, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.9156 - accuracy: 0.6622 - val_loss: 0.8167 - val_accuracy: 0.7014\n",
      "Epoch 8/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.7488 - accuracy: 0.8125\n",
      "Epoch 8: val_loss improved from 0.81669 to 0.79247, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.8722 - accuracy: 0.6966 - val_loss: 0.7925 - val_accuracy: 0.7122\n",
      "Epoch 9/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.7482 - accuracy: 0.8125\n",
      "Epoch 9: val_loss improved from 0.79247 to 0.76797, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.8312 - accuracy: 0.7010 - val_loss: 0.7680 - val_accuracy: 0.7158\n",
      "Epoch 10/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6475 - accuracy: 0.7500\n",
      "Epoch 10: val_loss improved from 0.76797 to 0.74210, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.7884 - accuracy: 0.7176 - val_loss: 0.7421 - val_accuracy: 0.7482\n",
      "Epoch 11/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.7078 - accuracy: 0.7500\n",
      "Epoch 11: val_loss improved from 0.74210 to 0.73460, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.7673 - accuracy: 0.7442 - val_loss: 0.7346 - val_accuracy: 0.7158\n",
      "Epoch 12/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6612 - accuracy: 0.7500\n",
      "Epoch 12: val_loss improved from 0.73460 to 0.70348, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 963us/step - loss: 0.7423 - accuracy: 0.7486 - val_loss: 0.7035 - val_accuracy: 0.7590\n",
      "Epoch 13/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6309 - accuracy: 0.8750\n",
      "Epoch 13: val_loss improved from 0.70348 to 0.68801, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 954us/step - loss: 0.7176 - accuracy: 0.7486 - val_loss: 0.6880 - val_accuracy: 0.7698\n",
      "Epoch 14/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6360 - accuracy: 0.8125\n",
      "Epoch 14: val_loss improved from 0.68801 to 0.67476, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 993us/step - loss: 0.7064 - accuracy: 0.7619 - val_loss: 0.6748 - val_accuracy: 0.7734\n",
      "Epoch 15/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.7584 - accuracy: 0.6250\n",
      "Epoch 15: val_loss did not improve from 0.67476\n",
      "57/57 [==============================] - 0s 884us/step - loss: 0.6819 - accuracy: 0.7608 - val_loss: 0.6774 - val_accuracy: 0.7590\n",
      "Epoch 16/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5902 - accuracy: 0.8125\n",
      "Epoch 16: val_loss improved from 0.67476 to 0.66199, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 941us/step - loss: 0.6598 - accuracy: 0.7841 - val_loss: 0.6620 - val_accuracy: 0.7950\n",
      "Epoch 17/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.8549 - accuracy: 0.6875\n",
      "Epoch 17: val_loss improved from 0.66199 to 0.63760, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 949us/step - loss: 0.6470 - accuracy: 0.7829 - val_loss: 0.6376 - val_accuracy: 0.7986\n",
      "Epoch 18/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.8592 - accuracy: 0.6875\n",
      "Epoch 18: val_loss improved from 0.63760 to 0.63227, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 950us/step - loss: 0.6375 - accuracy: 0.7807 - val_loss: 0.6323 - val_accuracy: 0.7986\n",
      "Epoch 19/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5399 - accuracy: 0.8750\n",
      "Epoch 19: val_loss did not improve from 0.63227\n",
      "57/57 [==============================] - 0s 863us/step - loss: 0.6207 - accuracy: 0.7841 - val_loss: 0.6414 - val_accuracy: 0.7986\n",
      "Epoch 20/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.7907 - accuracy: 0.7500\n",
      "Epoch 20: val_loss improved from 0.63227 to 0.62459, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 927us/step - loss: 0.6138 - accuracy: 0.7852 - val_loss: 0.6246 - val_accuracy: 0.8058\n",
      "Epoch 21/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3901 - accuracy: 0.9375\n",
      "Epoch 21: val_loss improved from 0.62459 to 0.61142, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 964us/step - loss: 0.6126 - accuracy: 0.7929 - val_loss: 0.6114 - val_accuracy: 0.8129\n",
      "Epoch 22/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4528 - accuracy: 0.8125\n",
      "Epoch 22: val_loss did not improve from 0.61142\n",
      "57/57 [==============================] - 0s 880us/step - loss: 0.6006 - accuracy: 0.7973 - val_loss: 0.6147 - val_accuracy: 0.8201\n",
      "Epoch 23/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4706 - accuracy: 0.9375\n",
      "Epoch 23: val_loss did not improve from 0.61142\n",
      "57/57 [==============================] - 0s 877us/step - loss: 0.5877 - accuracy: 0.7962 - val_loss: 0.6118 - val_accuracy: 0.8094\n",
      "Epoch 24/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6722 - accuracy: 0.6875\n",
      "Epoch 24: val_loss did not improve from 0.61142\n",
      "57/57 [==============================] - 0s 861us/step - loss: 0.5808 - accuracy: 0.7896 - val_loss: 0.6132 - val_accuracy: 0.8165\n",
      "Epoch 25/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3843 - accuracy: 0.9375\n",
      "Epoch 25: val_loss improved from 0.61142 to 0.61120, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 946us/step - loss: 0.5768 - accuracy: 0.7984 - val_loss: 0.6112 - val_accuracy: 0.8058\n",
      "Epoch 26/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4161 - accuracy: 0.9375\n",
      "Epoch 26: val_loss improved from 0.61120 to 0.59436, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 965us/step - loss: 0.5578 - accuracy: 0.8073 - val_loss: 0.5944 - val_accuracy: 0.8165\n",
      "Epoch 27/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4072 - accuracy: 0.8125\n",
      "Epoch 27: val_loss improved from 0.59436 to 0.57771, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 963us/step - loss: 0.5633 - accuracy: 0.8151 - val_loss: 0.5777 - val_accuracy: 0.8058\n",
      "Epoch 28/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4378 - accuracy: 0.8125\n",
      "Epoch 28: val_loss improved from 0.57771 to 0.56970, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 952us/step - loss: 0.5539 - accuracy: 0.8095 - val_loss: 0.5697 - val_accuracy: 0.8022\n",
      "Epoch 29/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4167 - accuracy: 1.0000\n",
      "Epoch 29: val_loss improved from 0.56970 to 0.56242, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 961us/step - loss: 0.5334 - accuracy: 0.8162 - val_loss: 0.5624 - val_accuracy: 0.8094\n",
      "Epoch 30/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6956 - accuracy: 0.8125\n",
      "Epoch 30: val_loss did not improve from 0.56242\n",
      "57/57 [==============================] - 0s 888us/step - loss: 0.5474 - accuracy: 0.8128 - val_loss: 0.5681 - val_accuracy: 0.8094\n",
      "Epoch 31/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4500 - accuracy: 0.8125\n",
      "Epoch 31: val_loss improved from 0.56242 to 0.56100, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 943us/step - loss: 0.5422 - accuracy: 0.8195 - val_loss: 0.5610 - val_accuracy: 0.8201\n",
      "Epoch 32/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5207 - accuracy: 0.7500\n",
      "Epoch 32: val_loss improved from 0.56100 to 0.55769, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 959us/step - loss: 0.5185 - accuracy: 0.8250 - val_loss: 0.5577 - val_accuracy: 0.8058\n",
      "Epoch 33/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3595 - accuracy: 0.8750\n",
      "Epoch 33: val_loss improved from 0.55769 to 0.54832, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 955us/step - loss: 0.5203 - accuracy: 0.8162 - val_loss: 0.5483 - val_accuracy: 0.8094\n",
      "Epoch 34/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5844 - accuracy: 0.7500\n",
      "Epoch 34: val_loss did not improve from 0.54832\n",
      "57/57 [==============================] - 0s 872us/step - loss: 0.5073 - accuracy: 0.8272 - val_loss: 0.5520 - val_accuracy: 0.8058\n",
      "Epoch 35/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3979 - accuracy: 0.8125\n",
      "Epoch 35: val_loss improved from 0.54832 to 0.54484, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 937us/step - loss: 0.5116 - accuracy: 0.8317 - val_loss: 0.5448 - val_accuracy: 0.8237\n",
      "Epoch 36/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3436 - accuracy: 1.0000\n",
      "Epoch 36: val_loss improved from 0.54484 to 0.53547, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 977us/step - loss: 0.5117 - accuracy: 0.8295 - val_loss: 0.5355 - val_accuracy: 0.8237\n",
      "Epoch 37/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4061 - accuracy: 0.8750\n",
      "Epoch 37: val_loss improved from 0.53547 to 0.52720, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 964us/step - loss: 0.4972 - accuracy: 0.8427 - val_loss: 0.5272 - val_accuracy: 0.8129\n",
      "Epoch 38/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.9342 - accuracy: 0.6250\n",
      "Epoch 38: val_loss did not improve from 0.52720\n",
      "57/57 [==============================] - 0s 886us/step - loss: 0.5132 - accuracy: 0.8184 - val_loss: 0.5476 - val_accuracy: 0.8165\n",
      "Epoch 39/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.2658 - accuracy: 1.0000\n",
      "Epoch 39: val_loss did not improve from 0.52720\n",
      "57/57 [==============================] - 0s 860us/step - loss: 0.4999 - accuracy: 0.8295 - val_loss: 0.5324 - val_accuracy: 0.8165\n",
      "Epoch 40/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4227 - accuracy: 0.8125\n",
      "Epoch 40: val_loss improved from 0.52720 to 0.52192, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 938us/step - loss: 0.5012 - accuracy: 0.8317 - val_loss: 0.5219 - val_accuracy: 0.8309\n",
      "Epoch 41/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3866 - accuracy: 0.8125\n",
      "Epoch 41: val_loss did not improve from 0.52192\n",
      "57/57 [==============================] - 0s 878us/step - loss: 0.4961 - accuracy: 0.8350 - val_loss: 0.5327 - val_accuracy: 0.8165\n",
      "Epoch 42/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3322 - accuracy: 0.8125\n",
      "Epoch 42: val_loss did not improve from 0.52192\n",
      "57/57 [==============================] - 0s 862us/step - loss: 0.4948 - accuracy: 0.8261 - val_loss: 0.5263 - val_accuracy: 0.8237\n",
      "Epoch 43/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3519 - accuracy: 0.8750\n",
      "Epoch 43: val_loss did not improve from 0.52192\n",
      "57/57 [==============================] - 0s 851us/step - loss: 0.4880 - accuracy: 0.8361 - val_loss: 0.5226 - val_accuracy: 0.8273\n",
      "Epoch 44/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6035 - accuracy: 0.8125\n",
      "Epoch 44: val_loss improved from 0.52192 to 0.52080, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 960us/step - loss: 0.4889 - accuracy: 0.8339 - val_loss: 0.5208 - val_accuracy: 0.8201\n",
      "Epoch 45/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3091 - accuracy: 0.8750\n",
      "Epoch 45: val_loss did not improve from 0.52080\n",
      "57/57 [==============================] - 0s 882us/step - loss: 0.4749 - accuracy: 0.8328 - val_loss: 0.5466 - val_accuracy: 0.8058\n",
      "Epoch 46/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3678 - accuracy: 0.8125\n",
      "Epoch 46: val_loss improved from 0.52080 to 0.50864, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 950us/step - loss: 0.4781 - accuracy: 0.8350 - val_loss: 0.5086 - val_accuracy: 0.8273\n",
      "Epoch 47/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6314 - accuracy: 0.6875\n",
      "Epoch 47: val_loss improved from 0.50864 to 0.50339, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 941us/step - loss: 0.4669 - accuracy: 0.8350 - val_loss: 0.5034 - val_accuracy: 0.8201\n",
      "Epoch 48/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4497 - accuracy: 0.7500\n",
      "Epoch 48: val_loss did not improve from 0.50339\n",
      "57/57 [==============================] - 0s 866us/step - loss: 0.4652 - accuracy: 0.8416 - val_loss: 0.5066 - val_accuracy: 0.8417\n",
      "Epoch 49/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4003 - accuracy: 0.8750\n",
      "Epoch 49: val_loss improved from 0.50339 to 0.50098, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 953us/step - loss: 0.4807 - accuracy: 0.8328 - val_loss: 0.5010 - val_accuracy: 0.8309\n",
      "Epoch 50/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.1982 - accuracy: 0.9375\n",
      "Epoch 50: val_loss did not improve from 0.50098\n",
      "57/57 [==============================] - 0s 857us/step - loss: 0.4678 - accuracy: 0.8394 - val_loss: 0.5043 - val_accuracy: 0.8345\n",
      "Epoch 51/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4600 - accuracy: 0.8125\n",
      "Epoch 51: val_loss improved from 0.50098 to 0.49097, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 940us/step - loss: 0.4562 - accuracy: 0.8361 - val_loss: 0.4910 - val_accuracy: 0.8381\n",
      "Epoch 52/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5888 - accuracy: 0.7500\n",
      "Epoch 52: val_loss improved from 0.49097 to 0.48506, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 958us/step - loss: 0.4621 - accuracy: 0.8472 - val_loss: 0.4851 - val_accuracy: 0.8309\n",
      "Epoch 53/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3921 - accuracy: 0.8125\n",
      "Epoch 53: val_loss improved from 0.48506 to 0.48176, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 963us/step - loss: 0.4583 - accuracy: 0.8383 - val_loss: 0.4818 - val_accuracy: 0.8381\n",
      "Epoch 54/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6615 - accuracy: 0.9375\n",
      "Epoch 54: val_loss did not improve from 0.48176\n",
      "57/57 [==============================] - 0s 864us/step - loss: 0.4421 - accuracy: 0.8472 - val_loss: 0.4892 - val_accuracy: 0.8345\n",
      "Epoch 55/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.7423 - accuracy: 0.6875\n",
      "Epoch 55: val_loss did not improve from 0.48176\n",
      "57/57 [==============================] - 0s 868us/step - loss: 0.4572 - accuracy: 0.8494 - val_loss: 0.4946 - val_accuracy: 0.8309\n",
      "Epoch 56/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.7802 - accuracy: 0.8125\n",
      "Epoch 56: val_loss improved from 0.48176 to 0.47794, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 941us/step - loss: 0.4414 - accuracy: 0.8439 - val_loss: 0.4779 - val_accuracy: 0.8273\n",
      "Epoch 57/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.9421 - accuracy: 0.6875\n",
      "Epoch 57: val_loss did not improve from 0.47794\n",
      "57/57 [==============================] - 0s 873us/step - loss: 0.4482 - accuracy: 0.8383 - val_loss: 0.4843 - val_accuracy: 0.8417\n",
      "Epoch 58/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4521 - accuracy: 0.8125\n",
      "Epoch 58: val_loss improved from 0.47794 to 0.46504, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 951us/step - loss: 0.4581 - accuracy: 0.8383 - val_loss: 0.4650 - val_accuracy: 0.8345\n",
      "Epoch 59/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.2649 - accuracy: 0.9375\n",
      "Epoch 59: val_loss did not improve from 0.46504\n",
      "57/57 [==============================] - 0s 874us/step - loss: 0.4341 - accuracy: 0.8450 - val_loss: 0.4671 - val_accuracy: 0.8489\n",
      "Epoch 60/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3605 - accuracy: 0.8125\n",
      "Epoch 60: val_loss did not improve from 0.46504\n",
      "57/57 [==============================] - 0s 851us/step - loss: 0.4310 - accuracy: 0.8527 - val_loss: 0.4711 - val_accuracy: 0.8273\n",
      "Epoch 61/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3049 - accuracy: 0.8750\n",
      "Epoch 61: val_loss did not improve from 0.46504\n",
      "57/57 [==============================] - 0s 861us/step - loss: 0.4355 - accuracy: 0.8538 - val_loss: 0.4707 - val_accuracy: 0.8381\n",
      "Epoch 62/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4340 - accuracy: 0.8750\n",
      "Epoch 62: val_loss did not improve from 0.46504\n",
      "57/57 [==============================] - 0s 867us/step - loss: 0.4254 - accuracy: 0.8461 - val_loss: 0.4888 - val_accuracy: 0.8345\n",
      "Epoch 63/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3945 - accuracy: 0.8125\n",
      "Epoch 63: val_loss did not improve from 0.46504\n",
      "57/57 [==============================] - 0s 853us/step - loss: 0.4325 - accuracy: 0.8472 - val_loss: 0.4706 - val_accuracy: 0.8453\n",
      "Epoch 64/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.2992 - accuracy: 0.8125\n",
      "Epoch 64: val_loss did not improve from 0.46504\n",
      "57/57 [==============================] - 0s 863us/step - loss: 0.4135 - accuracy: 0.8571 - val_loss: 0.4771 - val_accuracy: 0.8381\n",
      "Epoch 65/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3273 - accuracy: 0.8750\n",
      "Epoch 65: val_loss did not improve from 0.46504\n",
      "57/57 [==============================] - 0s 870us/step - loss: 0.4318 - accuracy: 0.8483 - val_loss: 0.4766 - val_accuracy: 0.8381\n",
      "Epoch 66/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.1962 - accuracy: 0.9375\n",
      "Epoch 66: val_loss did not improve from 0.46504\n",
      "57/57 [==============================] - 0s 888us/step - loss: 0.4249 - accuracy: 0.8494 - val_loss: 0.4788 - val_accuracy: 0.8273\n",
      "Epoch 67/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3522 - accuracy: 0.8750\n",
      "Epoch 67: val_loss improved from 0.46504 to 0.45798, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 946us/step - loss: 0.4265 - accuracy: 0.8461 - val_loss: 0.4580 - val_accuracy: 0.8345\n",
      "Epoch 68/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.1368 - accuracy: 1.0000\n",
      "Epoch 68: val_loss did not improve from 0.45798\n",
      "57/57 [==============================] - 0s 874us/step - loss: 0.4175 - accuracy: 0.8627 - val_loss: 0.4651 - val_accuracy: 0.8345\n",
      "Epoch 69/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.2821 - accuracy: 0.8750\n",
      "Epoch 69: val_loss improved from 0.45798 to 0.44945, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 955us/step - loss: 0.4093 - accuracy: 0.8483 - val_loss: 0.4495 - val_accuracy: 0.8489\n",
      "Epoch 70/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4667 - accuracy: 0.7500\n",
      "Epoch 70: val_loss did not improve from 0.44945\n",
      "57/57 [==============================] - 0s 865us/step - loss: 0.4062 - accuracy: 0.8560 - val_loss: 0.4598 - val_accuracy: 0.8381\n",
      "Epoch 71/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6199 - accuracy: 0.7500\n",
      "Epoch 71: val_loss did not improve from 0.44945\n",
      "57/57 [==============================] - 0s 883us/step - loss: 0.4146 - accuracy: 0.8527 - val_loss: 0.4598 - val_accuracy: 0.8453\n",
      "Epoch 72/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5363 - accuracy: 0.6875\n",
      "Epoch 72: val_loss did not improve from 0.44945\n",
      "57/57 [==============================] - 0s 857us/step - loss: 0.4071 - accuracy: 0.8505 - val_loss: 0.4572 - val_accuracy: 0.8417\n",
      "Epoch 73/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5242 - accuracy: 0.8125\n",
      "Epoch 73: val_loss did not improve from 0.44945\n",
      "57/57 [==============================] - 0s 866us/step - loss: 0.4039 - accuracy: 0.8660 - val_loss: 0.4680 - val_accuracy: 0.8489\n",
      "Epoch 74/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.1402 - accuracy: 1.0000\n",
      "Epoch 74: val_loss did not improve from 0.44945\n",
      "57/57 [==============================] - 0s 859us/step - loss: 0.4113 - accuracy: 0.8549 - val_loss: 0.4640 - val_accuracy: 0.8417\n",
      "Epoch 75/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4339 - accuracy: 0.8750\n",
      "Epoch 75: val_loss improved from 0.44945 to 0.44631, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 950us/step - loss: 0.4114 - accuracy: 0.8516 - val_loss: 0.4463 - val_accuracy: 0.8345\n",
      "Epoch 76/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.1538 - accuracy: 1.0000\n",
      "Epoch 76: val_loss improved from 0.44631 to 0.44455, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 990us/step - loss: 0.3978 - accuracy: 0.8594 - val_loss: 0.4446 - val_accuracy: 0.8489\n",
      "Epoch 77/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3626 - accuracy: 0.8750\n",
      "Epoch 77: val_loss improved from 0.44455 to 0.44272, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 951us/step - loss: 0.4125 - accuracy: 0.8594 - val_loss: 0.4427 - val_accuracy: 0.8417\n",
      "Epoch 78/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3015 - accuracy: 0.8125\n",
      "Epoch 78: val_loss did not improve from 0.44272\n",
      "57/57 [==============================] - 0s 868us/step - loss: 0.3930 - accuracy: 0.8505 - val_loss: 0.4462 - val_accuracy: 0.8417\n",
      "Epoch 79/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5041 - accuracy: 0.8125\n",
      "Epoch 79: val_loss did not improve from 0.44272\n",
      "57/57 [==============================] - 0s 861us/step - loss: 0.3888 - accuracy: 0.8571 - val_loss: 0.4514 - val_accuracy: 0.8453\n",
      "Epoch 80/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4270 - accuracy: 0.8125\n",
      "Epoch 80: val_loss improved from 0.44272 to 0.43869, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 937us/step - loss: 0.3924 - accuracy: 0.8627 - val_loss: 0.4387 - val_accuracy: 0.8453\n",
      "Epoch 81/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4733 - accuracy: 0.7500\n",
      "Epoch 81: val_loss improved from 0.43869 to 0.43807, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 955us/step - loss: 0.3797 - accuracy: 0.8583 - val_loss: 0.4381 - val_accuracy: 0.8525\n",
      "Epoch 82/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3022 - accuracy: 0.9375\n",
      "Epoch 82: val_loss improved from 0.43807 to 0.43160, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 955us/step - loss: 0.3981 - accuracy: 0.8516 - val_loss: 0.4316 - val_accuracy: 0.8453\n",
      "Epoch 83/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4598 - accuracy: 0.7500\n",
      "Epoch 83: val_loss did not improve from 0.43160\n",
      "57/57 [==============================] - 0s 875us/step - loss: 0.4006 - accuracy: 0.8560 - val_loss: 0.4476 - val_accuracy: 0.8453\n",
      "Epoch 84/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.1680 - accuracy: 1.0000\n",
      "Epoch 84: val_loss did not improve from 0.43160\n",
      "57/57 [==============================] - 0s 885us/step - loss: 0.3621 - accuracy: 0.8616 - val_loss: 0.4372 - val_accuracy: 0.8381\n",
      "Epoch 85/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.2032 - accuracy: 0.8750\n",
      "Epoch 85: val_loss did not improve from 0.43160\n",
      "57/57 [==============================] - 0s 866us/step - loss: 0.4020 - accuracy: 0.8571 - val_loss: 0.4369 - val_accuracy: 0.8561\n",
      "Epoch 86/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.8825 - accuracy: 0.7500\n",
      "Epoch 86: val_loss did not improve from 0.43160\n",
      "57/57 [==============================] - 0s 848us/step - loss: 0.3870 - accuracy: 0.8616 - val_loss: 0.4348 - val_accuracy: 0.8345\n",
      "Epoch 87/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3564 - accuracy: 0.8750\n",
      "Epoch 87: val_loss did not improve from 0.43160\n",
      "57/57 [==============================] - 0s 865us/step - loss: 0.3703 - accuracy: 0.8638 - val_loss: 0.4358 - val_accuracy: 0.8525\n",
      "Epoch 88/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3932 - accuracy: 0.9375\n",
      "Epoch 88: val_loss improved from 0.43160 to 0.43149, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.3967 - accuracy: 0.8505 - val_loss: 0.4315 - val_accuracy: 0.8597\n",
      "Epoch 89/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5209 - accuracy: 0.7500\n",
      "Epoch 89: val_loss improved from 0.43149 to 0.42932, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 962us/step - loss: 0.3903 - accuracy: 0.8560 - val_loss: 0.4293 - val_accuracy: 0.8525\n",
      "Epoch 90/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.1784 - accuracy: 0.9375\n",
      "Epoch 90: val_loss improved from 0.42932 to 0.42000, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 960us/step - loss: 0.3681 - accuracy: 0.8638 - val_loss: 0.4200 - val_accuracy: 0.8597\n",
      "Epoch 91/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4602 - accuracy: 0.8750\n",
      "Epoch 91: val_loss did not improve from 0.42000\n",
      "57/57 [==============================] - 0s 874us/step - loss: 0.3847 - accuracy: 0.8660 - val_loss: 0.4412 - val_accuracy: 0.8489\n",
      "Epoch 92/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3086 - accuracy: 0.9375\n",
      "Epoch 92: val_loss did not improve from 0.42000\n",
      "57/57 [==============================] - 0s 856us/step - loss: 0.3972 - accuracy: 0.8693 - val_loss: 0.4249 - val_accuracy: 0.8597\n",
      "Epoch 93/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.2707 - accuracy: 0.8750\n",
      "Epoch 93: val_loss did not improve from 0.42000\n",
      "57/57 [==============================] - 0s 878us/step - loss: 0.3861 - accuracy: 0.8627 - val_loss: 0.4338 - val_accuracy: 0.8669\n",
      "Epoch 94/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6441 - accuracy: 0.8125\n",
      "Epoch 94: val_loss did not improve from 0.42000\n",
      "57/57 [==============================] - 0s 866us/step - loss: 0.3716 - accuracy: 0.8671 - val_loss: 0.4323 - val_accuracy: 0.8525\n",
      "Epoch 95/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.2202 - accuracy: 1.0000\n",
      "Epoch 95: val_loss did not improve from 0.42000\n",
      "57/57 [==============================] - 0s 847us/step - loss: 0.3625 - accuracy: 0.8660 - val_loss: 0.4223 - val_accuracy: 0.8489\n",
      "Epoch 96/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.0472 - accuracy: 1.0000\n",
      "Epoch 96: val_loss did not improve from 0.42000\n",
      "57/57 [==============================] - 0s 865us/step - loss: 0.3761 - accuracy: 0.8616 - val_loss: 0.4374 - val_accuracy: 0.8417\n",
      "Epoch 97/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3192 - accuracy: 0.8750\n",
      "Epoch 97: val_loss improved from 0.42000 to 0.41706, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 950us/step - loss: 0.3747 - accuracy: 0.8649 - val_loss: 0.4171 - val_accuracy: 0.8525\n",
      "Epoch 98/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.2032 - accuracy: 1.0000\n",
      "Epoch 98: val_loss did not improve from 0.41706\n",
      "57/57 [==============================] - 0s 860us/step - loss: 0.3698 - accuracy: 0.8649 - val_loss: 0.4195 - val_accuracy: 0.8597\n",
      "Epoch 99/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4595 - accuracy: 0.8125\n",
      "Epoch 99: val_loss did not improve from 0.41706\n",
      "57/57 [==============================] - 0s 861us/step - loss: 0.3717 - accuracy: 0.8671 - val_loss: 0.4201 - val_accuracy: 0.8561\n",
      "Epoch 100/100\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3373 - accuracy: 0.8750\n",
      "Epoch 100: val_loss improved from 0.41706 to 0.41371, saving model to imu_cnn_model.h5\n",
      "57/57 [==============================] - 0s 949us/step - loss: 0.3732 - accuracy: 0.8560 - val_loss: 0.4137 - val_accuracy: 0.8597\n",
      "--- Training Finished ---\n",
      "\n",
      "--- Evaluating Model on Test Set ---\n",
      "Test Loss: 0.5207\n",
      "Test Accuracy: 0.8325\n",
      "7/7 [==============================] - 0s 472us/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        jump       0.92      0.87      0.89        39\n",
      "        null       0.77      0.70      0.73        57\n",
      "        spin       0.92      0.90      0.91        62\n",
      "       weave       0.75      0.86      0.80        51\n",
      "\n",
      "    accuracy                           0.83       209\n",
      "   macro avg       0.84      0.83      0.83       209\n",
      "weighted avg       0.84      0.83      0.83       209\n",
      "\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved to 'imu_scaler.pkl'\n",
      "\n",
      "--- Normalization Constants for C++ ---\n",
      "const float FEATURE_MEANS[NUM_FEATURES] = {\n",
      "    1.20960010e-01, 7.94896026e-01, -5.09449145e+00, 7.74947705e-04, 1.49835118e-01, -4.48857635e-01, -3.34466937e+00, -4.57668365e+00, -2.40626536e+01  // ax, ay, az, gx, gy, gz, mx, my, mz means\n",
      "};\n",
      "const float FEATURE_SCALES[NUM_FEATURES] = {\n",
      "    7.87378730e+00, 1.02205258e+01, 7.44162342e+00, 2.05283428e+00, 3.50854654e+00, 2.14226314e+00, 3.05463829e+01, 3.34312034e+01, 2.82488909e+01  // ax, ay, az, gx, gy, gz, mx, my, mz scales (std dev)\n",
      "};\n",
      "Label encoder saved to 'imu_label_encoder.pkl'\n",
      "Class mapping: {0: 'jump', 1: 'null', 2: 'spin', 3: 'weave'}\n",
      "\n",
      "--- Converting Model to TensorFlow Lite ---\n",
      "Successfully loaded best model from 'imu_cnn_model.h5' for conversion.\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpesswi5sf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpesswi5sf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully converted to TensorFlow Lite format.\n",
      "TFLite model saved to 'imu_model.tflite' (7056 bytes)\n",
      "\n",
      "To convert to C array, run in your terminal:\n",
      "xxd -i imu_model.tflite > imu_model.h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 14:49:02.219699: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-04-03 14:49:02.219713: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-04-03 14:49:02.219798: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpesswi5sf\n",
      "2025-04-03 14:49:02.220371: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-04-03 14:49:02.220376: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpesswi5sf\n",
      "2025-04-03 14:49:02.222024: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-04-03 14:49:02.238284: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpesswi5sf\n",
      "2025-04-03 14:49:02.242934: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 23135 microseconds.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "# Highlight: Import TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout,\n",
    "    GlobalAveragePooling1D, GlobalAveragePooling2D, Reshape # Keep Reshape for now\n",
    ")\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# --- Configuration (Keep as before) ---\n",
    "PICKLE_FILE = 'labeled_imu_segments.pkl'\n",
    "MODEL_SAVE_PATH = 'imu_cnn_model.h5'     # <-- USE .h5 extension\n",
    "SCALER_SAVE_PATH = 'imu_scaler.pkl'\n",
    "LABEL_ENCODER_SAVE_PATH = 'imu_label_encoder.pkl'\n",
    "N_CLASSES = 4\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "VALIDATION_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.15\n",
    "\n",
    "# --- Functions (Keep load_pickle_data, prepare_data, scale_data) ---\n",
    "\n",
    "def load_pickle_data(filepath):\n",
    "    \"\"\"Loads data from a pickle file.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"Successfully loaded data from '{filepath}' ({len(data)} segments)\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pickle file '{filepath}': {e}\")\n",
    "        return None\n",
    "\n",
    "def prepare_data(labeled_segments):\n",
    "    \"\"\"Separates data and labels, encodes labels.\"\"\"\n",
    "    if not labeled_segments:\n",
    "        return None, None, None\n",
    "\n",
    "    # Check data integrity (shape)\n",
    "    first_shape = labeled_segments[0]['data'].shape\n",
    "    if len(first_shape) != 2 or first_shape[1] != 9:\n",
    "         print(f\"Error: Unexpected data shape in first segment: {first_shape}. Expected (timesteps, 9)\")\n",
    "         return None, None, None\n",
    "    timesteps = first_shape[0]\n",
    "    features = first_shape[1]\n",
    "    print(f\"Detected segment shape: ({timesteps}, {features})\")\n",
    "\n",
    "    X = []\n",
    "    y_labels = []\n",
    "    for segment in labeled_segments:\n",
    "         # Ensure all data has the expected shape before adding\n",
    "         if segment['data'].shape == (timesteps, features):\n",
    "              X.append(segment['data'])\n",
    "              y_labels.append(segment['label'])\n",
    "         else:\n",
    "              print(f\"Warning: Skipping segment with unexpected shape {segment['data'].shape}\")\n",
    "\n",
    "\n",
    "    if not X:\n",
    "         print(\"Error: No valid data segments found after shape check.\")\n",
    "         return None, None, None\n",
    "\n",
    "    X = np.array(X) # Shape: (num_samples, timesteps, features)\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y_labels)\n",
    "    y_one_hot = to_categorical(y_encoded, num_classes=N_CLASSES)\n",
    "\n",
    "    print(\"Labels encoded.\")\n",
    "    print(\"Class mapping:\", {i: c for i, c in enumerate(label_encoder.classes_)})\n",
    "\n",
    "    return X, y_one_hot, label_encoder, timesteps, features\n",
    "\n",
    "def scale_data(X_train, X_val, X_test, n_features):\n",
    "    \"\"\"Fits StandardScaler on training data and transforms all sets.\"\"\"\n",
    "    orig_shape_train = X_train.shape\n",
    "    orig_shape_val = X_val.shape\n",
    "    orig_shape_test = X_test.shape\n",
    "\n",
    "    # Reshape to 2D for scaler\n",
    "    X_train_reshaped = X_train.reshape(-1, n_features)\n",
    "    X_val_reshaped = X_val.reshape(-1, n_features)\n",
    "    X_test_reshaped = X_test.reshape(-1, n_features)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train_reshaped)\n",
    "\n",
    "    X_train_scaled_reshaped = scaler.transform(X_train_reshaped)\n",
    "    X_val_scaled_reshaped = scaler.transform(X_val_reshaped)\n",
    "    X_test_scaled_reshaped = scaler.transform(X_test_reshaped)\n",
    "\n",
    "    # Reshape back to original 3D shape\n",
    "    X_train_scaled = X_train_scaled_reshaped.reshape(orig_shape_train)\n",
    "    X_val_scaled = X_val_scaled_reshaped.reshape(orig_shape_val)\n",
    "    X_test_scaled = X_test_scaled_reshaped.reshape(orig_shape_test)\n",
    "\n",
    "    print(\"Data scaled using StandardScaler (fitted on training data).\")\n",
    "\n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, scaler\n",
    "\n",
    "\n",
    "def build_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Builds a CNN model without an explicit Reshape layer. This version assumes\n",
    "    the input data is already in the shape (1, timesteps, features).\n",
    "    \"\"\"\n",
    "    print(f\"Building model with Input shape: {input_shape}\")  # input_shape should be (1, timesteps, features), e.g., (1,45,9)\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),  # e.g., (1,45,9)\n",
    "        tf.keras.layers.Conv2D(filters=8, kernel_size=(1, 5), activation='relu', padding='same', name='conv2d_1'),\n",
    "        Dropout(0.2),\n",
    "        tf.keras.layers.Conv2D(filters=16, kernel_size=(1, 5), activation='relu', padding='same', name='conv2d_2'),\n",
    "        Dropout(0.25),\n",
    "        GlobalAveragePooling2D(name='global_avg_pool'),\n",
    "        Dense(num_classes, activation='softmax', name='output_dense')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # <<<--- GPU Check (Keep as before) --->>>\n",
    "    print(\"TensorFlow Version:\", tf.__version__)\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(f\"Found {len(gpus)} Physical GPU(s), {len(logical_gpus)} Logical GPU(s)\")\n",
    "            print(\"GPU acceleration will be used.\")\n",
    "        except RuntimeError as e:\n",
    "            print(\"GPU Memory Growth Error:\", e)\n",
    "            print(\"Will proceed without memory growth setting.\")\n",
    "    else:\n",
    "        print(\"!!!!!!!!!!!!!!!!!\")\n",
    "        print(\"!!! No GPU found using `tf.config.list_physical_devices('GPU')`.\")\n",
    "        print(\"!!! Training will proceed on CPU.\")\n",
    "        print(\"!!! Check TensorFlow installation, NVIDIA drivers, CUDA, and cuDNN.\")\n",
    "        print(\"!!!!!!!!!!!!!!!!!\")\n",
    "    # <<<--- GPU Check End --->>>\n",
    "\n",
    "\n",
    "    # 1. Load Data\n",
    "    labeled_segments = load_pickle_data(PICKLE_FILE)\n",
    "    if labeled_segments is None:\n",
    "        exit()\n",
    "\n",
    "    # 2. Prepare Data (Separate X/y, Encode Labels)\n",
    "    X, y_one_hot, label_encoder, timesteps, features = prepare_data(labeled_segments)\n",
    "    if X is None:\n",
    "        exit()\n",
    "    # X shape here is (num_samples, timesteps, features)\n",
    "\n",
    "    # <<<--- *** CHANGE 1: Define the target INPUT_SHAPE for the model *** --->>>\n",
    "    # This is the shape TFLM Conv2D op expects for Conv1D input\n",
    "    INPUT_SHAPE = (1, timesteps, features)\n",
    "    print(f\"Target model input shape: {INPUT_SHAPE}\")\n",
    "\n",
    "    # Get integer labels for stratification and class weights\n",
    "    y_integers = np.argmax(y_one_hot, axis=1)\n",
    "\n",
    "    # 3. Split Data (Train, Validation, Test)\n",
    "    # Data is still (num_samples, timesteps, features) here\n",
    "    X_train_val, X_test, y_train_val, y_test, y_int_train_val, y_int_test = train_test_split(\n",
    "        X, y_one_hot, y_integers,\n",
    "        test_size=TEST_SPLIT, random_state=42, stratify=y_integers\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val,\n",
    "        test_size=VALIDATION_SPLIT / (1.0 - TEST_SPLIT), random_state=42,\n",
    "        stratify=y_int_train_val\n",
    "    )\n",
    "    print(f\"Data split (before reshape): Train={X_train.shape}, Validation={X_val.shape}, Test={X_test.shape}\")\n",
    "\n",
    "    # 4. Scale/Normalize Data\n",
    "    # Data remains (num_samples, timesteps, features) after scaling\n",
    "    X_train_scaled, X_val_scaled, X_test_scaled, scaler = scale_data(\n",
    "        X_train, X_val, X_test, features\n",
    "    )\n",
    "\n",
    "    # <<<--- *** CHANGE 2: Reshape the scaled data to match the INPUT_SHAPE *** --->>>\n",
    "    print(\"Reshaping data to add dimension for TFLM Conv2D compatibility...\")\n",
    "    X_train_final = X_train_scaled.reshape((-1, 1, timesteps, features))\n",
    "    X_val_final = X_val_scaled.reshape((-1, 1, timesteps, features))\n",
    "    X_test_final = X_test_scaled.reshape((-1, 1, timesteps, features))\n",
    "    print(f\"Data shapes after reshape: Train={X_train_final.shape}, Validation={X_val_final.shape}, Test={X_test_final.shape}\")\n",
    "\n",
    "\n",
    "    # 5. Calculate Class Weights for Imbalance\n",
    "    y_train_integers = np.argmax(y_train, axis=1)\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced', classes=np.unique(y_train_integers), y=y_train_integers\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    print(f\"Calculated Class Weights: {class_weight_dict}\")\n",
    "\n",
    "    # 6. Build Model (Pass the new INPUT_SHAPE)\n",
    "    model = build_cnn_model(INPUT_SHAPE, N_CLASSES)\n",
    "    model.summary() # Check model architecture, notice the input/output shapes\n",
    "\n",
    "    # 7. Define Callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, verbose=1, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint(MODEL_SAVE_PATH, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "    # 8. Train Model (Use the reshaped data X_train_final, X_val_final)\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    history = model.fit(\n",
    "        X_train_final, y_train, # <<<--- Use reshaped data\n",
    "        epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_val_final, y_val), # <<<--- Use reshaped data\n",
    "        callbacks=[early_stopping, model_checkpoint],\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"--- Training Finished ---\")\n",
    "\n",
    "    # 9. Evaluate Model (Use the reshaped test data X_test_final)\n",
    "    print(\"\\n--- Evaluating Model on Test Set ---\")\n",
    "    loss, accuracy = model.evaluate(X_test_final, y_test, verbose=0) # <<<--- Use reshaped data\n",
    "    print(f\"Test Loss: {loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Generate predictions and classification report\n",
    "    y_pred_probs = model.predict(X_test_final) # <<<--- Use reshaped data\n",
    "    y_pred_int = np.argmax(y_pred_probs, axis=1)\n",
    "    y_test_int = np.argmax(y_test, axis=1) # Already have y_int_test, use that\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_int_test, y_pred_int, target_names=label_encoder.classes_))\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_int_test, y_pred_int)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # 10. Save Scaler and Label Encoder\n",
    "    with open(SCALER_SAVE_PATH, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(f\"Scaler saved to '{SCALER_SAVE_PATH}'\")\n",
    "    # Print scaler means/scales needed for C++ code\n",
    "    print(\"\\n--- Normalization Constants for C++ ---\")\n",
    "    print(\"const float FEATURE_MEANS[NUM_FEATURES] = {\")\n",
    "    print(\"   \", \", \".join([f\"{m:.8e}\" for m in scaler.mean_]), \" // ax, ay, az, gx, gy, gz, mx, my, mz means\")\n",
    "    print(\"};\")\n",
    "    print(\"const float FEATURE_SCALES[NUM_FEATURES] = {\")\n",
    "    print(\"   \", \", \".join([f\"{s:.8e}\" for s in scaler.scale_]), \" // ax, ay, az, gx, gy, gz, mx, my, mz scales (std dev)\")\n",
    "    print(\"};\")\n",
    "\n",
    "    with open(LABEL_ENCODER_SAVE_PATH, 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    print(f\"Label encoder saved to '{LABEL_ENCODER_SAVE_PATH}'\")\n",
    "    print(\"Class mapping:\", {i: c for i, c in enumerate(label_encoder.classes_)})\n",
    "\n",
    "\n",
    "    # --- TensorFlow Lite Conversion ---\n",
    "    print(\"\\n--- Converting Model to TensorFlow Lite ---\")\n",
    "\n",
    "    # Load the best saved model\n",
    "    # It's better to load the saved model to ensure we convert the best one\n",
    "    try:\n",
    "        best_model = tf.keras.models.load_model(MODEL_SAVE_PATH)\n",
    "        print(f\"Successfully loaded best model from '{MODEL_SAVE_PATH}' for conversion.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading saved model '{MODEL_SAVE_PATH}': {e}\")\n",
    "        print(\"Attempting to convert the model currently in memory (might not be the best one).\")\n",
    "        best_model = model # Fallback to model in memory\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
    "    # Optional: Add optimizations (e.g., quantization) if needed later\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    # Define a representative dataset generator (using reshaped validation data)\n",
    "    def representative_dataset_gen():\n",
    "        # Use a subset for efficiency, ensure it's float32 and correctly shaped\n",
    "        # Using X_val_final which is already scaled and reshaped\n",
    "        num_samples = X_val_final.shape[0]\n",
    "        samples_to_use = min(100, num_samples) # Use up to 100 samples\n",
    "        indices = np.random.choice(num_samples, samples_to_use, replace=False)\n",
    "        for i in indices:\n",
    "            # Yield samples one by one, adding batch dimension (size 1)\n",
    "             yield [X_val_final[i:i+1].astype(np.float32)] # <<<--- Use reshaped data\n",
    "\n",
    "    # Enable representative dataset for quantization (if needed, requires optimizations)\n",
    "    # If you only use DEFAULT optimization without quantization, you might not need this.\n",
    "    # But it's good practice if you might quantize later.\n",
    "    # converter.representative_dataset = representative_dataset_gen\n",
    "    # Uncomment below if doing INT8 quantization\n",
    "    # converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    # converter.inference_input_type = tf.int8 # or tf.uint8\n",
    "    # converter.inference_output_type = tf.int8 # or tf.uint8\n",
    "\n",
    "    try:\n",
    "        tflite_model = converter.convert()\n",
    "        print(\"Model successfully converted to TensorFlow Lite format.\")\n",
    "\n",
    "        # Save the TFLite model\n",
    "        tflite_model_path = 'imu_model.tflite'\n",
    "        with open(tflite_model_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"TFLite model saved to '{tflite_model_path}' ({len(tflite_model)} bytes)\")\n",
    "\n",
    "        # Optional: Convert to C array using xxd\n",
    "        print(\"\\nTo convert to C array, run in your terminal:\")\n",
    "        print(f\"xxd -i {tflite_model_path} > imu_model.h\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!! Error during TFLite conversion: {e}\")\n",
    "        print(\"Check model layers, input shapes, and TensorFlow version compatibility.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting TFLite model to C source file using xxd...\n",
      "C array saved to: imu_model.h\n",
      "You can now include this .h file in your Arduino project.\n"
     ]
    }
   ],
   "source": [
    "# --- Optional: Convert TFLite model to C array using xxd ---\n",
    "# This is often the easiest way to include the model in Arduino projects\n",
    "print(\"\\nConverting TFLite model to C source file using xxd...\")\n",
    "# Make sure xxd is available in your system PATH (common on Linux/macOS, installable on Windows)\n",
    "try:\n",
    "    # Generate C array file (e.g., imu_model.cc)\n",
    "    os.system(f\"xxd -i {TFLITE_MODEL_PATH} > {TFLITE_MODEL_CC_PATH}\")\n",
    "\n",
    "    # Optional: Read the C file and modify variable name for clarity\n",
    "    with open(TFLITE_MODEL_CC_PATH, 'r') as f:\n",
    "        c_content = f.read()\n",
    "    # Replace the default variable name generated by xxd\n",
    "    # (adjust 'imu_model_tflite' if your filename is different)\n",
    "    c_content = c_content.replace('unsigned char imu_model_tflite[] = {',\n",
    "                                  'const unsigned char g_imu_model_data[] = {')\n",
    "    c_content = c_content.replace('unsigned int imu_model_tflite_len = ',\n",
    "                                  'const unsigned int g_imu_model_data_len = ')\n",
    "    # Add header guard and include\n",
    "    c_output = f\"#ifndef IMU_MODEL_DATA_H_\\n#define IMU_MODEL_DATA_H_\\n\\n{c_content}\\n#endif // IMU_MODEL_DATA_H_\\n\"\n",
    "\n",
    "    with open(TFLITE_MODEL_CC_PATH.replace('.cc', '.h'), 'w') as f: # Save as .h\n",
    "         f.write(c_output)\n",
    "\n",
    "    print(f\"C array saved to: {TFLITE_MODEL_CC_PATH.replace('.cc', '.h')}\")\n",
    "    print(\"You can now include this .h file in your Arduino project.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nError: 'xxd' command not found.\")\n",
    "    print(\"Please install 'xxd' (part of vim-common or similar packages)\")\n",
    "    print(f\"or manually convert '{TFLITE_MODEL_PATH}' to a C array.\")\n",
    "except Exception as e:\n",
    "     print(f\"\\nError during xxd conversion or C file modification: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 07:21:01.494136: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-04 07:21:01.495231: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-04 07:21:01.516324: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-04 07:21:01.516795: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-04 07:21:01.889644: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.13.1\n",
      "!!! No GPU found. Training on CPU. !!!\n",
      "Successfully loaded data from 'labeled_imu_segments.pkl' (1390 segments)\n",
      "Detected segment shape: (45, 9)\n",
      "Labels encoded.\n",
      "Class mapping: {0: 'jump', 1: 'null', 2: 'spin', 3: 'weave'}\n",
      "Using Explicit 2D Input shape for Keras model: (1, 45, 9)\n",
      "\n",
      "--- Debugging y_one_hot ---\n",
      "Type of y_one_hot: <class 'numpy.ndarray'>\n",
      "Shape of y_one_hot before argmax: (1390, 4)\n",
      "Dimensions (ndim) of y_one_hot before argmax: 2\n",
      "Sample of y_one_hot (first 5 rows):\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n",
      "--- End Debugging ---\n",
      "\n",
      "Shape of y_integers after argmax: (1390,)\n",
      "Data split (Augmented): Train=903, Val=278, Test=209\n",
      "Data scaled using StandardScaler (fitted on training data).\n",
      "Reshaping data to 4D (batch, 1, timesteps, features)...\n",
      "Data shapes after reshape: Train=(903, 1, 45, 9), Val=(278, 1, 45, 9), Test=(209, 1, 45, 9)\n",
      "Class Weights: {0: 1.3357988165680474, 1: 0.9176829268292683, 2: 0.8392193308550185, 3: 1.0308219178082192}\n",
      "Building explicit 2D model with Input shape: (1, 45, 9)\n",
      "\n",
      "--- Applying Pruning Wrapper ---\n",
      "Pruning end step calculated: 6720 (entire training)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 07:21:02.227022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-04 07:21:02.227180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-04 07:21:02.249922: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model wrapped for pruning:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_conv2d  (None, 1, 45, 8)          730       \n",
      " _1 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_dropou  (None, 1, 45, 8)          1         \n",
      " t (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d  (None, 1, 45, 16)         1298      \n",
      " _2 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_dropou  (None, 1, 45, 16)         1         \n",
      " t_1 (PruneLowMagnitude)                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_global  (None, 1, 1, 16)          1         \n",
      " _avg_pool_2d (PruneLowMagn                                      \n",
      " itude)                                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_flatte  (None, 16)                1         \n",
      " n_output (PruneLowMagnitud                                      \n",
      " e)                                                              \n",
      "                                                                 \n",
      " prune_low_magnitude_output  (None, 4)                 134       \n",
      " _dense (PruneLowMagnitude)                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2166 (8.49 KB)\n",
      "Trainable params: 1092 (4.27 KB)\n",
      "Non-trainable params: 1074 (4.22 KB)\n",
      "_________________________________________________________________\n",
      "\n",
      "TensorBoard logs will be saved to: logs/pruning/20250404-072102\n",
      "Run `tensorboard --logdir logs/pruning` in your terminal to view.\n",
      "\n",
      "--- Starting Pruning-Aware Training ---\n",
      "Epoch 1/120\n",
      " 1/57 [..............................] - ETA: 51s - loss: 1.4524 - accuracy: 0.2500WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_end` time: 0.0031s). Check your callbacks.\n",
      "53/57 [==========================>...] - ETA: 0s - loss: 1.3930 - accuracy: 0.2866 \n",
      "Epoch 1: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 1s 4ms/step - loss: 1.3905 - accuracy: 0.2879 - val_loss: 1.3283 - val_accuracy: 0.4173\n",
      "Epoch 2/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 1.2413 - accuracy: 0.4375\n",
      "Epoch 2: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 1.3011 - accuracy: 0.3654 - val_loss: 1.3176 - val_accuracy: 0.3201\n",
      "Epoch 3/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 1.2709 - accuracy: 0.3750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aden/anaconda3/envs/TFenv/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 1.2622 - accuracy: 0.4297 - val_loss: 1.2163 - val_accuracy: 0.4964\n",
      "Epoch 4/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 1.2181 - accuracy: 0.4375\n",
      "Epoch 4: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 1.2009 - accuracy: 0.4684 - val_loss: 1.1469 - val_accuracy: 0.4964\n",
      "Epoch 5/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 1.1756 - accuracy: 0.6250\n",
      "Epoch 5: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 1.1492 - accuracy: 0.4806 - val_loss: 1.1144 - val_accuracy: 0.5108\n",
      "Epoch 6/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 1.2334 - accuracy: 0.3750\n",
      "Epoch 6: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 1.1051 - accuracy: 0.4817 - val_loss: 1.0671 - val_accuracy: 0.5647\n",
      "Epoch 7/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 1.0095 - accuracy: 0.5625\n",
      "Epoch 7: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 1.0626 - accuracy: 0.5404 - val_loss: 1.0385 - val_accuracy: 0.5827\n",
      "Epoch 8/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.8767 - accuracy: 0.6875\n",
      "Epoch 8: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 1.0219 - accuracy: 0.5748 - val_loss: 1.0027 - val_accuracy: 0.5971\n",
      "Epoch 9/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 1.1374 - accuracy: 0.4375\n",
      "Epoch 9: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.9869 - accuracy: 0.6035 - val_loss: 0.9616 - val_accuracy: 0.6511\n",
      "Epoch 10/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.8486 - accuracy: 0.5625\n",
      "Epoch 10: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.9553 - accuracy: 0.6168 - val_loss: 0.9325 - val_accuracy: 0.6835\n",
      "Epoch 11/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.8614 - accuracy: 0.6250\n",
      "Epoch 11: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.9226 - accuracy: 0.6390 - val_loss: 0.9034 - val_accuracy: 0.6942\n",
      "Epoch 12/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.9541 - accuracy: 0.6250\n",
      "Epoch 12: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.8995 - accuracy: 0.6489 - val_loss: 0.8806 - val_accuracy: 0.7122\n",
      "Epoch 13/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6380 - accuracy: 0.6875\n",
      "Epoch 13: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.8669 - accuracy: 0.6578 - val_loss: 0.8515 - val_accuracy: 0.7302\n",
      "Epoch 14/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.7546 - accuracy: 0.6875\n",
      "Epoch 14: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.8551 - accuracy: 0.6822 - val_loss: 0.8344 - val_accuracy: 0.7410\n",
      "Epoch 15/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.7507 - accuracy: 0.7500\n",
      "Epoch 15: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.8350 - accuracy: 0.6800 - val_loss: 0.8176 - val_accuracy: 0.7374\n",
      "Epoch 16/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 1.0176 - accuracy: 0.5625\n",
      "Epoch 16: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.8030 - accuracy: 0.7010 - val_loss: 0.7970 - val_accuracy: 0.7482\n",
      "Epoch 17/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 1.0006 - accuracy: 0.6250\n",
      "Epoch 17: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.7887 - accuracy: 0.7121 - val_loss: 0.7832 - val_accuracy: 0.7626\n",
      "Epoch 18/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.7323 - accuracy: 0.8125\n",
      "Epoch 18: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.7783 - accuracy: 0.7220 - val_loss: 0.7646 - val_accuracy: 0.7698\n",
      "Epoch 19/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5906 - accuracy: 0.8125\n",
      "Epoch 19: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.7589 - accuracy: 0.7209 - val_loss: 0.7495 - val_accuracy: 0.7770\n",
      "Epoch 20/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6083 - accuracy: 0.6875\n",
      "Epoch 20: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.7830 - accuracy: 0.6622 - val_loss: 0.7509 - val_accuracy: 0.7626\n",
      "Epoch 21/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.7396 - accuracy: 0.6875\n",
      "Epoch 21: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.7571 - accuracy: 0.7209 - val_loss: 0.7356 - val_accuracy: 0.7662\n",
      "Epoch 22/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.8217 - accuracy: 0.6250\n",
      "Epoch 22: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.7402 - accuracy: 0.7276 - val_loss: 0.7253 - val_accuracy: 0.7662\n",
      "Epoch 23/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.9329 - accuracy: 0.6875\n",
      "Epoch 23: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.7349 - accuracy: 0.7165 - val_loss: 0.7148 - val_accuracy: 0.7554\n",
      "Epoch 24/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6900 - accuracy: 0.6875\n",
      "Epoch 24: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.7178 - accuracy: 0.7353 - val_loss: 0.7043 - val_accuracy: 0.7626\n",
      "Epoch 25/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.8437 - accuracy: 0.6250\n",
      "Epoch 25: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.7066 - accuracy: 0.7298 - val_loss: 0.7029 - val_accuracy: 0.7842\n",
      "Epoch 26/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.7673 - accuracy: 0.6875\n",
      "Epoch 26: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.7109 - accuracy: 0.7331 - val_loss: 0.6945 - val_accuracy: 0.7770\n",
      "Epoch 27/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6148 - accuracy: 0.8125\n",
      "Epoch 27: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6966 - accuracy: 0.7364 - val_loss: 0.6884 - val_accuracy: 0.7878\n",
      "Epoch 28/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6978 - accuracy: 0.7500\n",
      "Epoch 28: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6891 - accuracy: 0.7409 - val_loss: 0.6772 - val_accuracy: 0.7914\n",
      "Epoch 29/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.8122 - accuracy: 0.6250\n",
      "Epoch 29: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6813 - accuracy: 0.7353 - val_loss: 0.6771 - val_accuracy: 0.7734\n",
      "Epoch 30/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.7001 - accuracy: 0.7500\n",
      "Epoch 30: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.7309 - val_loss: 0.6753 - val_accuracy: 0.7842\n",
      "Epoch 31/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5287 - accuracy: 0.6875\n",
      "Epoch 31: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6848 - accuracy: 0.7453 - val_loss: 0.6654 - val_accuracy: 0.7842\n",
      "Epoch 32/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6759 - accuracy: 0.7500\n",
      "Epoch 32: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6774 - accuracy: 0.7409 - val_loss: 0.6767 - val_accuracy: 0.7698\n",
      "Epoch 33/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.7198 - accuracy: 0.6875\n",
      "Epoch 33: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6696 - accuracy: 0.7309 - val_loss: 0.6569 - val_accuracy: 0.7806\n",
      "Epoch 34/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6701 - accuracy: 0.8750\n",
      "Epoch 34: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6720 - accuracy: 0.7431 - val_loss: 0.6568 - val_accuracy: 0.7770\n",
      "Epoch 35/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5615 - accuracy: 0.7500\n",
      "Epoch 35: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6646 - accuracy: 0.7386 - val_loss: 0.6547 - val_accuracy: 0.7842\n",
      "Epoch 36/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.8985 - accuracy: 0.7500\n",
      "Epoch 36: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6998 - accuracy: 0.7187 - val_loss: 0.6571 - val_accuracy: 0.7734\n",
      "Epoch 37/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.9258 - accuracy: 0.6250\n",
      "Epoch 37: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6629 - accuracy: 0.7276 - val_loss: 0.6488 - val_accuracy: 0.7698\n",
      "Epoch 38/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5386 - accuracy: 0.8125\n",
      "Epoch 38: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6691 - accuracy: 0.7497 - val_loss: 0.6484 - val_accuracy: 0.7518\n",
      "Epoch 39/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5724 - accuracy: 0.8750\n",
      "Epoch 39: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6436 - accuracy: 0.7497 - val_loss: 0.6463 - val_accuracy: 0.7698\n",
      "Epoch 40/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6284 - accuracy: 0.6875\n",
      "Epoch 40: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6516 - accuracy: 0.7409 - val_loss: 0.6333 - val_accuracy: 0.7770\n",
      "Epoch 41/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4735 - accuracy: 0.9375\n",
      "Epoch 41: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6583 - accuracy: 0.7298 - val_loss: 0.6346 - val_accuracy: 0.7770\n",
      "Epoch 42/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6668 - accuracy: 0.8125\n",
      "Epoch 42: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6405 - accuracy: 0.7486 - val_loss: 0.6324 - val_accuracy: 0.7842\n",
      "Epoch 43/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4824 - accuracy: 0.8750\n",
      "Epoch 43: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6284 - accuracy: 0.7508 - val_loss: 0.6294 - val_accuracy: 0.7770\n",
      "Epoch 44/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6930 - accuracy: 0.6875\n",
      "Epoch 44: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6444 - accuracy: 0.7475 - val_loss: 0.6328 - val_accuracy: 0.7770\n",
      "Epoch 45/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.9744 - accuracy: 0.5625\n",
      "Epoch 45: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6385 - accuracy: 0.7497 - val_loss: 0.6264 - val_accuracy: 0.7842\n",
      "Epoch 46/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6258 - accuracy: 0.7500\n",
      "Epoch 46: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6550 - accuracy: 0.7420 - val_loss: 0.6363 - val_accuracy: 0.7986\n",
      "Epoch 47/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6497 - accuracy: 0.6875\n",
      "Epoch 47: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6463 - accuracy: 0.7542 - val_loss: 0.6284 - val_accuracy: 0.7986\n",
      "Epoch 48/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.8617 - accuracy: 0.8750\n",
      "Epoch 48: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6364 - accuracy: 0.7597 - val_loss: 0.6187 - val_accuracy: 0.7842\n",
      "Epoch 49/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5462 - accuracy: 0.7500\n",
      "Epoch 49: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6404 - accuracy: 0.7586 - val_loss: 0.6213 - val_accuracy: 0.7806\n",
      "Epoch 50/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5992 - accuracy: 0.6875\n",
      "Epoch 50: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6443 - accuracy: 0.7398 - val_loss: 0.6089 - val_accuracy: 0.7806\n",
      "Epoch 51/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5851 - accuracy: 0.8125\n",
      "Epoch 51: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6316 - accuracy: 0.7486 - val_loss: 0.6288 - val_accuracy: 0.7806\n",
      "Epoch 52/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.7910 - accuracy: 0.8125\n",
      "Epoch 52: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6231 - accuracy: 0.7697 - val_loss: 0.6068 - val_accuracy: 0.7842\n",
      "Epoch 53/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4760 - accuracy: 0.7500\n",
      "Epoch 53: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6262 - accuracy: 0.7708 - val_loss: 0.6075 - val_accuracy: 0.7662\n",
      "Epoch 54/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5479 - accuracy: 0.7500\n",
      "Epoch 54: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6086 - accuracy: 0.7575 - val_loss: 0.5994 - val_accuracy: 0.7770\n",
      "Epoch 55/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 1.0000 - accuracy: 0.6250\n",
      "Epoch 55: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6127 - accuracy: 0.7553 - val_loss: 0.5988 - val_accuracy: 0.7770\n",
      "Epoch 56/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3785 - accuracy: 0.8750\n",
      "Epoch 56: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6086 - accuracy: 0.7619 - val_loss: 0.5947 - val_accuracy: 0.7878\n",
      "Epoch 57/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6107 - accuracy: 0.6875\n",
      "Epoch 57: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6176 - accuracy: 0.7564 - val_loss: 0.5917 - val_accuracy: 0.7950\n",
      "Epoch 58/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4718 - accuracy: 0.8125\n",
      "Epoch 58: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6273 - accuracy: 0.7519 - val_loss: 0.6460 - val_accuracy: 0.7446\n",
      "Epoch 59/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5171 - accuracy: 0.8125\n",
      "Epoch 59: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6298 - accuracy: 0.7519 - val_loss: 0.5923 - val_accuracy: 0.7770\n",
      "Epoch 60/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6016 - accuracy: 0.6875\n",
      "Epoch 60: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6042 - accuracy: 0.7685 - val_loss: 0.5839 - val_accuracy: 0.7842\n",
      "Epoch 61/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.7328 - accuracy: 0.6875\n",
      "Epoch 61: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5987 - accuracy: 0.7697 - val_loss: 0.5859 - val_accuracy: 0.7950\n",
      "Epoch 62/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6868 - accuracy: 0.6875\n",
      "Epoch 62: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5968 - accuracy: 0.7597 - val_loss: 0.5895 - val_accuracy: 0.7770\n",
      "Epoch 63/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6203 - accuracy: 0.7500\n",
      "Epoch 63: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6023 - accuracy: 0.7807 - val_loss: 0.5858 - val_accuracy: 0.7878\n",
      "Epoch 64/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4912 - accuracy: 0.8125\n",
      "Epoch 64: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6240 - accuracy: 0.7508 - val_loss: 0.5774 - val_accuracy: 0.7878\n",
      "Epoch 65/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4987 - accuracy: 0.6875\n",
      "Epoch 65: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5930 - accuracy: 0.7763 - val_loss: 0.5768 - val_accuracy: 0.7950\n",
      "Epoch 66/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.8318 - accuracy: 0.5625\n",
      "Epoch 66: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5897 - accuracy: 0.7685 - val_loss: 0.5754 - val_accuracy: 0.7914\n",
      "Epoch 67/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3882 - accuracy: 0.8750\n",
      "Epoch 67: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6216 - accuracy: 0.7663 - val_loss: 0.6213 - val_accuracy: 0.7554\n",
      "Epoch 68/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5995 - accuracy: 0.6875\n",
      "Epoch 68: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5990 - accuracy: 0.7608 - val_loss: 0.5766 - val_accuracy: 0.7914\n",
      "Epoch 69/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6487 - accuracy: 0.7500\n",
      "Epoch 69: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5984 - accuracy: 0.7807 - val_loss: 0.5783 - val_accuracy: 0.7878\n",
      "Epoch 70/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 1.0555 - accuracy: 0.5625\n",
      "Epoch 70: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6007 - accuracy: 0.7763 - val_loss: 0.5775 - val_accuracy: 0.7878\n",
      "Epoch 71/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.9031 - accuracy: 0.7500\n",
      "Epoch 71: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5980 - accuracy: 0.7630 - val_loss: 0.5771 - val_accuracy: 0.7914\n",
      "Epoch 72/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6968 - accuracy: 0.7500\n",
      "Epoch 72: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5901 - accuracy: 0.7785 - val_loss: 0.5727 - val_accuracy: 0.7842\n",
      "Epoch 73/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3722 - accuracy: 0.8125\n",
      "Epoch 73: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6007 - accuracy: 0.7652 - val_loss: 0.5747 - val_accuracy: 0.7914\n",
      "Epoch 74/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3865 - accuracy: 0.8750\n",
      "Epoch 74: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5763 - accuracy: 0.7807 - val_loss: 0.5686 - val_accuracy: 0.7950\n",
      "Epoch 75/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3294 - accuracy: 0.8750\n",
      "Epoch 75: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5967 - accuracy: 0.7663 - val_loss: 0.5674 - val_accuracy: 0.7914\n",
      "Epoch 76/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5964 - accuracy: 0.6875\n",
      "Epoch 76: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5688 - accuracy: 0.7807 - val_loss: 0.5686 - val_accuracy: 0.7914\n",
      "Epoch 77/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6802 - accuracy: 0.7500\n",
      "Epoch 77: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5906 - accuracy: 0.7796 - val_loss: 0.5679 - val_accuracy: 0.7950\n",
      "Epoch 78/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 1.0033 - accuracy: 0.6250\n",
      "Epoch 78: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5800 - accuracy: 0.7874 - val_loss: 0.5563 - val_accuracy: 0.8022\n",
      "Epoch 79/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5141 - accuracy: 0.7500\n",
      "Epoch 79: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5971 - accuracy: 0.7730 - val_loss: 0.6650 - val_accuracy: 0.7230\n",
      "Epoch 80/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.8553 - accuracy: 0.6250\n",
      "Epoch 80: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.6501 - accuracy: 0.7287 - val_loss: 0.5669 - val_accuracy: 0.8022\n",
      "Epoch 81/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.7376 - accuracy: 0.7500\n",
      "Epoch 81: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5778 - accuracy: 0.7708 - val_loss: 0.5686 - val_accuracy: 0.7986\n",
      "Epoch 82/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3662 - accuracy: 0.9375\n",
      "Epoch 82: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5850 - accuracy: 0.7719 - val_loss: 0.5594 - val_accuracy: 0.8058\n",
      "Epoch 83/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5868 - accuracy: 0.8125\n",
      "Epoch 83: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5936 - accuracy: 0.7564 - val_loss: 0.5604 - val_accuracy: 0.7986\n",
      "Epoch 84/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6818 - accuracy: 0.6875\n",
      "Epoch 84: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5948 - accuracy: 0.7752 - val_loss: 0.5598 - val_accuracy: 0.8022\n",
      "Epoch 85/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3108 - accuracy: 0.9375\n",
      "Epoch 85: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5742 - accuracy: 0.7829 - val_loss: 0.5624 - val_accuracy: 0.8022\n",
      "Epoch 86/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3634 - accuracy: 0.8750\n",
      "Epoch 86: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5900 - accuracy: 0.7641 - val_loss: 0.5644 - val_accuracy: 0.7986\n",
      "Epoch 87/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 1.1065 - accuracy: 0.4375\n",
      "Epoch 87: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5696 - accuracy: 0.7874 - val_loss: 0.5535 - val_accuracy: 0.8094\n",
      "Epoch 88/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5470 - accuracy: 0.6875\n",
      "Epoch 88: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5897 - accuracy: 0.7796 - val_loss: 0.5584 - val_accuracy: 0.8094\n",
      "Epoch 89/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4011 - accuracy: 0.8125\n",
      "Epoch 89: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5951 - accuracy: 0.7719 - val_loss: 0.5602 - val_accuracy: 0.7986\n",
      "Epoch 90/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5272 - accuracy: 0.6875\n",
      "Epoch 90: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5844 - accuracy: 0.7829 - val_loss: 0.5543 - val_accuracy: 0.8022\n",
      "Epoch 91/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.9609 - accuracy: 0.6250\n",
      "Epoch 91: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5846 - accuracy: 0.7553 - val_loss: 0.5522 - val_accuracy: 0.8058\n",
      "Epoch 92/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5185 - accuracy: 0.8125\n",
      "Epoch 92: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5556 - accuracy: 0.7885 - val_loss: 0.5478 - val_accuracy: 0.8094\n",
      "Epoch 93/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.7278 - accuracy: 0.7500\n",
      "Epoch 93: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5967 - accuracy: 0.7763 - val_loss: 0.5437 - val_accuracy: 0.8129\n",
      "Epoch 94/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4450 - accuracy: 0.8125\n",
      "Epoch 94: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5702 - accuracy: 0.7774 - val_loss: 0.5385 - val_accuracy: 0.8165\n",
      "Epoch 95/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4925 - accuracy: 0.8125\n",
      "Epoch 95: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5538 - accuracy: 0.7940 - val_loss: 0.5398 - val_accuracy: 0.8129\n",
      "Epoch 96/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.2926 - accuracy: 0.8125\n",
      "Epoch 96: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5703 - accuracy: 0.7885 - val_loss: 0.5371 - val_accuracy: 0.8129\n",
      "Epoch 97/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5297 - accuracy: 0.7500\n",
      "Epoch 97: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5831 - accuracy: 0.7719 - val_loss: 0.5345 - val_accuracy: 0.8165\n",
      "Epoch 98/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6196 - accuracy: 0.8125\n",
      "Epoch 98: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5697 - accuracy: 0.7730 - val_loss: 0.5376 - val_accuracy: 0.8129\n",
      "Epoch 99/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4229 - accuracy: 0.8750\n",
      "Epoch 99: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5552 - accuracy: 0.7818 - val_loss: 0.5349 - val_accuracy: 0.8165\n",
      "Epoch 100/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5373 - accuracy: 0.7500\n",
      "Epoch 100: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5721 - accuracy: 0.7807 - val_loss: 0.5391 - val_accuracy: 0.8201\n",
      "Epoch 101/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5337 - accuracy: 0.7500\n",
      "Epoch 101: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5709 - accuracy: 0.7818 - val_loss: 0.5352 - val_accuracy: 0.8129\n",
      "Epoch 102/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.2664 - accuracy: 0.9375\n",
      "Epoch 102: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5685 - accuracy: 0.7885 - val_loss: 0.5383 - val_accuracy: 0.8129\n",
      "Epoch 103/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3905 - accuracy: 0.8750\n",
      "Epoch 103: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5671 - accuracy: 0.7763 - val_loss: 0.5332 - val_accuracy: 0.8165\n",
      "Epoch 104/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.4488 - accuracy: 0.8750\n",
      "Epoch 104: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5651 - accuracy: 0.7763 - val_loss: 0.5314 - val_accuracy: 0.8201\n",
      "Epoch 105/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5060 - accuracy: 0.8125\n",
      "Epoch 105: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5545 - accuracy: 0.7940 - val_loss: 0.5285 - val_accuracy: 0.8201\n",
      "Epoch 106/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5624 - accuracy: 0.7500\n",
      "Epoch 106: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5670 - accuracy: 0.7929 - val_loss: 0.5280 - val_accuracy: 0.8237\n",
      "Epoch 107/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.2679 - accuracy: 0.9375\n",
      "Epoch 107: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5491 - accuracy: 0.7918 - val_loss: 0.5316 - val_accuracy: 0.8201\n",
      "Epoch 108/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3840 - accuracy: 0.8125\n",
      "Epoch 108: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5717 - accuracy: 0.7874 - val_loss: 0.5326 - val_accuracy: 0.8237\n",
      "Epoch 109/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.8064 - accuracy: 0.7500\n",
      "Epoch 109: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5580 - accuracy: 0.7929 - val_loss: 0.5296 - val_accuracy: 0.8201\n",
      "Epoch 110/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6084 - accuracy: 0.8750\n",
      "Epoch 110: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5645 - accuracy: 0.7896 - val_loss: 0.5275 - val_accuracy: 0.8237\n",
      "Epoch 111/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5427 - accuracy: 0.7500\n",
      "Epoch 111: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5517 - accuracy: 0.7863 - val_loss: 0.5289 - val_accuracy: 0.8165\n",
      "Epoch 112/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.3458 - accuracy: 0.9375\n",
      "Epoch 112: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5851 - accuracy: 0.7774 - val_loss: 0.5242 - val_accuracy: 0.8201\n",
      "Epoch 113/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5028 - accuracy: 0.8750\n",
      "Epoch 113: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5587 - accuracy: 0.7885 - val_loss: 0.5237 - val_accuracy: 0.8237\n",
      "Epoch 114/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6130 - accuracy: 0.7500\n",
      "Epoch 114: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5689 - accuracy: 0.7874 - val_loss: 0.5309 - val_accuracy: 0.8237\n",
      "Epoch 115/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 1.1790 - accuracy: 0.5000\n",
      "Epoch 115: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5562 - accuracy: 0.7907 - val_loss: 0.5259 - val_accuracy: 0.8201\n",
      "Epoch 116/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.2240 - accuracy: 0.9375\n",
      "Epoch 116: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5651 - accuracy: 0.7852 - val_loss: 0.5268 - val_accuracy: 0.8237\n",
      "Epoch 117/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.7261 - accuracy: 0.8750\n",
      "Epoch 117: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5570 - accuracy: 0.7796 - val_loss: 0.5232 - val_accuracy: 0.8237\n",
      "Epoch 118/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.2175 - accuracy: 1.0000\n",
      "Epoch 118: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5595 - accuracy: 0.7874 - val_loss: 0.5171 - val_accuracy: 0.8273\n",
      "Epoch 119/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.6124 - accuracy: 0.6250\n",
      "Epoch 119: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5578 - accuracy: 0.7885 - val_loss: 0.5182 - val_accuracy: 0.8273\n",
      "Epoch 120/120\n",
      " 1/57 [..............................] - ETA: 0s - loss: 0.5090 - accuracy: 0.8125\n",
      "Epoch 120: saving model to imu_cnn_model_pruned_pruning.h5\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.5392 - accuracy: 0.7940 - val_loss: 0.5148 - val_accuracy: 0.8237\n",
      "--- Pruning-Aware Training Finished ---\n",
      "\n",
      "--- Stripping Pruning Wrappers ---\n",
      "Loading last saved pruning checkpoint...\n",
      "Warning: Could not load pruning checkpoint, stripping model currently in memory: Unknown layer: 'PruneLowMagnitude'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n",
      "Model after stripping wrappers:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_1 (Conv2D)           (None, 1, 45, 8)          368       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1, 45, 8)          0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 1, 45, 16)         656       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1, 45, 16)         0         \n",
      "                                                                 \n",
      " global_avg_pool_2d (Averag  (None, 1, 1, 16)          0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " flatten_output (Flatten)    (None, 16)                0         \n",
      "                                                                 \n",
      " output_dense (Dense)        (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1092 (4.27 KB)\n",
      "Trainable params: 1092 (4.27 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Final stripped model saved to imu_cnn_model_pruned.h5\n",
      "\n",
      "--- Evaluating Final Stripped Model on Test Set ---\n",
      "Test Loss (Stripped Model): 0.5157\n",
      "Test Accuracy (Stripped Model): 0.8086\n",
      "7/7 [==============================] - 0s 499us/step\n",
      "\n",
      "Classification Report (Stripped Model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        jump       0.92      0.92      0.92        39\n",
      "        null       0.84      0.74      0.79        57\n",
      "        spin       0.94      0.71      0.81        62\n",
      "       weave       0.64      0.92      0.76        51\n",
      "\n",
      "    accuracy                           0.81       209\n",
      "   macro avg       0.84      0.82      0.82       209\n",
      "weighted avg       0.84      0.81      0.81       209\n",
      "\n",
      "\n",
      "Confusion Matrix (Stripped Model):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved to 'imu_scaler_pruned.pkl'\n",
      "Label encoder saved to 'imu_label_encoder_pruned.pkl'\n",
      "\n",
      "--- Normalization Constants for C++ ---\n",
      "// Constants generated from scaler: imu_scaler_pruned.pkl\n",
      "const float FEATURE_MEANS[NUM_FEATURES] = {\n",
      "    -5.59217423e-02, 1.16576129e+00, -5.07616267e+00, -4.03543743e-02, 2.04697674e-01, -4.87264427e-01, -3.60985407e+00, -2.80538058e+00, -2.33431101e+01  // ax, ay, az,... means\n",
      "};\n",
      "const float FEATURE_SCALES[NUM_FEATURES] = {\n",
      "    7.91380806e+00, 1.02338530e+01, 7.61295134e+00, 2.09523183e+00, 3.55369937e+00, 2.12962856e+00, 3.01921328e+01, 3.48021246e+01, 2.83512230e+01  // ax, ay, az,... scales (std dev)\n",
      "};\n",
      "const char* LABELS[NUM_CLASSES] = { \"jump\", \"null\", \"spin\", \"weave\" };\n",
      "#define NUM_TIMESTEPS 45\n",
      "#define NUM_FEATURES 9\n",
      "#define NUM_CLASSES 4\n",
      "\n",
      "--- Converting Final Stripped Model to TensorFlow Lite ---\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpaggzeesp/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpaggzeesp/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stripped model successfully converted to TensorFlow Lite format.\n",
      "Pruned TFLite model saved to 'imu_model_pruned.tflite' (7224 bytes)\n",
      "\n",
      "Converting 'imu_model_pruned.tflite' to 'imu_model.h' using xxd...\n",
      "Successfully created C header file 'imu_model.h'\n",
      "IMPORTANT: Manually edit 'imu_model.h' to ensure the array\n",
      "           is named 'g_imu_model_data' and the length variable is present.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 07:21:15.017412: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-04-04 07:21:15.017425: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-04-04 07:21:15.017573: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpaggzeesp\n",
      "2025-04-04 07:21:15.018108: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-04-04 07:21:15.018114: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/tmpaggzeesp\n",
      "2025-04-04 07:21:15.019590: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2025-04-04 07:21:15.019883: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-04-04 07:21:15.031252: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpaggzeesp\n",
      "2025-04-04 07:21:15.034658: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 17084 microseconds.\n",
      "2025-04-04 07:21:15.039505: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "# --- TFMOT Import ---\n",
    "import tensorflow_model_optimization as tfmot\n",
    "# --- End TFMOT Import ---\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, AveragePooling2D, Flatten, Dense, Dropout\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard # Added TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import datetime # For TensorBoard logs\n",
    "\n",
    "# --- Configuration (Update filenames) ---\n",
    "PICKLE_FILE = 'labeled_imu_segments.pkl' # <<< USE AUGMENTED DATA\n",
    "MODEL_SAVE_PATH = 'imu_cnn_model_pruned.h5'      # <<< New model name\n",
    "SCALER_SAVE_PATH = 'imu_scaler_pruned.pkl'       # <<< New scaler name\n",
    "LABEL_ENCODER_SAVE_PATH = 'imu_label_encoder_pruned.pkl' # <<< New encoder name\n",
    "TFLITE_MODEL_PATH = 'imu_model_pruned.tflite'      # <<< New tflite name\n",
    "CHEADER_FILE_PATH = 'imu_model.h'                # <<< Keep output C header name\n",
    "N_CLASSES = 4\n",
    "EPOCHS = 120 # <<< May need slightly more epochs for pruning\n",
    "BATCH_SIZE = 16\n",
    "VALIDATION_SPLIT = 0.2 # Use fraction of the *loaded* data\n",
    "TEST_SPLIT = 0.15      # Use fraction of the *original* (pre-split) data size concept\n",
    "\n",
    "# --- Pruning Configuration ---\n",
    "PRUNING_INITIAL_SPARSITY = 0.50 # Start with 50% sparsity\n",
    "PRUNING_FINAL_SPARSITY = 0.80   # Target 80% sparsity\n",
    "PRUNING_BEGIN_STEP = 0          # Start pruning immediately (or set to e.g., 2 epochs * steps_per_epoch)\n",
    "PRUNING_END_STEP = -1           # End pruning at the end of training (or set specific step)\n",
    "PRUNING_FREQUENCY = 100         # How often to update pruning mask (steps)\n",
    "\n",
    "# --- Functions (load_pickle_data, prepare_data, scale_data - modify prepare_data slightly) ---\n",
    "def load_pickle_data(filepath):\n",
    "    # (Same as before)\n",
    "    if not os.path.exists(filepath): print(f\"Error: File not found at {filepath}\"); return None\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f: data = pickle.load(f)\n",
    "        print(f\"Successfully loaded data from '{filepath}' ({len(data)} segments)\")\n",
    "        return data\n",
    "    except Exception as e: print(f\"Error loading pickle file '{filepath}': {e}\"); return None\n",
    "\n",
    "def prepare_data(labeled_segments):\n",
    "    # (Same as before, just extracts data/labels)\n",
    "    if not labeled_segments: return None, None, None, None, None\n",
    "    first_shape = labeled_segments[0]['data'].shape\n",
    "    if len(first_shape) != 2 or first_shape[1] != 9:\n",
    "         print(f\"Error: Unexpected data shape: {first_shape}. Expected (timesteps, 9)\"); return None, None, None, None, None\n",
    "    timesteps = first_shape[0]; features = first_shape[1]\n",
    "    print(f\"Detected segment shape: ({timesteps}, {features})\")\n",
    "    X = []; y_labels = []\n",
    "    for segment in labeled_segments:\n",
    "         if segment['data'].shape == (timesteps, features):\n",
    "              X.append(segment['data']); y_labels.append(segment['label'])\n",
    "         else: print(f\"Warning: Skipping segment shape {segment['data'].shape}\")\n",
    "    if not X: print(\"Error: No valid segments.\"); return None, None, None, None, None\n",
    "    X = np.array(X); label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y_labels)\n",
    "    y_one_hot = to_categorical(y_encoded, num_classes=N_CLASSES)\n",
    "    print(\"Labels encoded.\"); print(\"Class mapping:\", dict(enumerate(label_encoder.classes_)))\n",
    "    return X, y_one_hot, label_encoder, timesteps, features\n",
    "\n",
    "def scale_data(X_train, X_val, X_test, n_features):\n",
    "   # --- IMPORTANT: Fit scaler ONLY on TRAINING data ---\n",
    "    orig_shape_train = X_train.shape; orig_shape_val = X_val.shape; orig_shape_test = X_test.shape\n",
    "    X_train_reshaped = X_train.reshape(-1, n_features)\n",
    "    # Only transform val/test, don't reshape them yet if they are None\n",
    "    X_val_reshaped = X_val.reshape(-1, n_features) if X_val is not None else None\n",
    "    X_test_reshaped = X_test.reshape(-1, n_features) if X_test is not None else None\n",
    "\n",
    "    scaler = StandardScaler(); scaler.fit(X_train_reshaped) # Fit ONLY on training data\n",
    "\n",
    "    X_train_scaled = scaler.transform(X_train_reshaped).reshape(orig_shape_train)\n",
    "    X_val_scaled = scaler.transform(X_val_reshaped).reshape(orig_shape_val) if X_val is not None else None\n",
    "    X_test_scaled = scaler.transform(X_test_reshaped).reshape(orig_shape_test) if X_test is not None else None\n",
    "\n",
    "    print(\"Data scaled using StandardScaler (fitted on training data).\");\n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, scaler\n",
    "\n",
    "# --- Model Definition (Explicit Conv2D - NO CHANGE NEEDED HERE) ---\n",
    "def build_cnn_model_explicit_2d(input_shape_2d, num_classes, timesteps):\n",
    "    # (Same as your previous explicit 2D model definition)\n",
    "    print(f\"Building explicit 2D model with Input shape: {input_shape_2d}\")\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape_2d, name='input_layer_2d'),\n",
    "        Conv2D(filters=8, kernel_size=(1, 5), activation='relu', padding='same', name='conv2d_1'),\n",
    "        Dropout(0.2),\n",
    "        Conv2D(filters=16, kernel_size=(1, 5), activation='relu', padding='same', name='conv2d_2'),\n",
    "        Dropout(0.25),\n",
    "        AveragePooling2D(pool_size=(1, timesteps), name='global_avg_pool_2d'),\n",
    "        Flatten(name='flatten_output'),\n",
    "        Dense(num_classes, activation='softmax', name='output_dense')\n",
    "    ])\n",
    "    return model # Return the standard Keras model\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # GPU Check (Keep as before)\n",
    "    # ... (GPU check code) ...\n",
    "    print(\"TensorFlow Version:\", tf.__version__)\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(f\"Found {len(gpus)} Physical GPU(s), {len(logical_gpus)} Logical GPU(s). GPU acceleration will be used.\")\n",
    "        except RuntimeError as e: print(\"GPU Memory Growth Error:\", e)\n",
    "    else: print(\"!!! No GPU found. Training on CPU. !!!\")\n",
    "\n",
    "\n",
    "    # 1. Load Data (Use the AUGMENTED file)\n",
    "    labeled_segments = load_pickle_data(PICKLE_FILE) # Load augmented data\n",
    "    if labeled_segments is None: exit()\n",
    "\n",
    "    # 2. Prepare Data\n",
    "    X, y_one_hot, label_encoder, timesteps, features = prepare_data(labeled_segments)\n",
    "    if X is None:\n",
    "        print(\"Exiting because prepare_data returned None for X.\")\n",
    "        exit()\n",
    "\n",
    "    # <<<--- Define the Explicit 2D Input Shape --- >>>\n",
    "    INPUT_SHAPE_2D = (1, timesteps, features)\n",
    "    print(f\"Using Explicit 2D Input shape for Keras model: {INPUT_SHAPE_2D}\")\n",
    "\n",
    "    # --- DEBUGGING ---\n",
    "    print(f\"\\n--- Debugging y_one_hot ---\")\n",
    "    print(f\"Type of y_one_hot: {type(y_one_hot)}\")\n",
    "    if isinstance(y_one_hot, np.ndarray):\n",
    "        print(f\"Shape of y_one_hot before argmax: {y_one_hot.shape}\")\n",
    "        print(f\"Dimensions (ndim) of y_one_hot before argmax: {y_one_hot.ndim}\")\n",
    "        print(f\"Sample of y_one_hot (first 5 rows):\\n{y_one_hot[:5]}\")\n",
    "    elif y_one_hot is None:\n",
    "        print(\"y_one_hot is None!\")\n",
    "    else:\n",
    "        print(\"y_one_hot is not a numpy array!\")\n",
    "    print(f\"--- End Debugging ---\\n\")\n",
    "    # --- END DEBUGGING ---\n",
    "\n",
    "    # Use try-except block to catch the error gracefully and print info\n",
    "    try:\n",
    "        y_integers = np.argmax(y_one_hot, axis=1) # <<< ERROR HERE\n",
    "        print(f\"Shape of y_integers after argmax: {y_integers.shape}\") # Check output shape\n",
    "    except AxisError as e:\n",
    "        print(f\"FATAL ERROR during np.argmax: {e}\")\n",
    "        if isinstance(y_one_hot, np.ndarray):\n",
    "            print(f\"Details: Error occurred with y_one_hot of shape={y_one_hot.shape}, ndim={y_one_hot.ndim}\")\n",
    "        exit() # Stop execution\n",
    "    except Exception as e: # Catch other potential errors\n",
    "        print(f\"An unexpected error occurred during np.argmax: {e}\")\n",
    "        exit()\n",
    "    # 3. Split Data (Split the *augmented* data)\n",
    "    # Stratify based on the integer labels\n",
    "    X_train, X_temp, y_train, y_temp, y_int_train, y_int_temp = train_test_split(\n",
    "        X, y_one_hot, y_integers,\n",
    "        test_size=(VALIDATION_SPLIT + TEST_SPLIT), # Combine val and test sizes initially\n",
    "        random_state=42,\n",
    "        stratify=y_integers # Stratify based on original labels\n",
    "    )\n",
    "    # Calculate split point for validation vs test from the temp set\n",
    "    relative_test_size = TEST_SPLIT / (VALIDATION_SPLIT + TEST_SPLIT)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=relative_test_size,\n",
    "        random_state=42,\n",
    "        stratify=y_int_temp # Stratify again\n",
    "    )\n",
    "    print(f\"Data split (Augmented): Train={X_train.shape[0]}, Val={X_val.shape[0]}, Test={X_test.shape[0]}\")\n",
    "\n",
    "\n",
    "    # 4. Scale Data (Fit scaler ONLY on X_train)\n",
    "    X_train_scaled, X_val_scaled, X_test_scaled, scaler = scale_data(\n",
    "        X_train, X_val, X_test, features\n",
    "    )\n",
    "\n",
    "    # 5. Reshape Data to 4D for the Model (ALL sets)\n",
    "    print(\"Reshaping data to 4D (batch, 1, timesteps, features)...\")\n",
    "    X_train_final = X_train_scaled.reshape((-1, 1, timesteps, features))\n",
    "    X_val_final = X_val_scaled.reshape((-1, 1, timesteps, features))\n",
    "    X_test_final = X_test_scaled.reshape((-1, 1, timesteps, features))\n",
    "    print(f\"Data shapes after reshape: Train={X_train_final.shape}, Val={X_val_final.shape}, Test={X_test_final.shape}\")\n",
    "\n",
    "\n",
    "    # 6. Calculate Class Weights (Based on Training Set)\n",
    "    y_train_integers = np.argmax(y_train, axis=1) # Use original y_train before reshape\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train_integers), y=y_train_integers)\n",
    "    class_weight_dict = dict(enumerate(class_weights)); print(f\"Class Weights: {class_weight_dict}\")\n",
    "\n",
    "    # 7. Build the standard Keras Model\n",
    "    model = build_cnn_model_explicit_2d(INPUT_SHAPE_2D, N_CLASSES, timesteps)\n",
    "\n",
    "    # --- Pruning Specific Steps ---\n",
    "    print(\"\\n--- Applying Pruning Wrapper ---\")\n",
    "    # Calculate pruning end step based on dataset size and epochs\n",
    "    num_train_samples = X_train_final.shape[0]\n",
    "    steps_per_epoch = num_train_samples // BATCH_SIZE\n",
    "    if PRUNING_END_STEP == -1: # If set to -1, prune for entire training duration\n",
    "         pruning_end_step = steps_per_epoch * EPOCHS\n",
    "         print(f\"Pruning end step calculated: {pruning_end_step} (entire training)\")\n",
    "    else:\n",
    "         pruning_end_step = PRUNING_END_STEP\n",
    "         print(f\"Using specified pruning end step: {pruning_end_step}\")\n",
    "\n",
    "    pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "            initial_sparsity=PRUNING_INITIAL_SPARSITY,\n",
    "            final_sparsity=PRUNING_FINAL_SPARSITY,\n",
    "            begin_step=PRUNING_BEGIN_STEP,\n",
    "            end_step=pruning_end_step,\n",
    "            frequency=PRUNING_FREQUENCY\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Apply the pruning wrapper\n",
    "    model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "    # Re-compile the model wrapped for pruning\n",
    "    model_for_pruning.compile(optimizer='adam',\n",
    "                              loss='categorical_crossentropy',\n",
    "                              metrics=['accuracy'])\n",
    "\n",
    "    print(\"Model wrapped for pruning:\")\n",
    "    model_for_pruning.summary() # Notice the prune_low_magnitude wrappers\n",
    "\n",
    "    # --- End Pruning Specific Steps ---\n",
    "\n",
    "\n",
    "    # 8. Define Callbacks (Add pruning callback and TensorBoard)\n",
    "    log_dir = \"logs/pruning/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    callbacks = [\n",
    "        tfmot.sparsity.keras.UpdatePruningStep(), # Required for pruning schedule\n",
    "        tfmot.sparsity.keras.PruningSummaries(log_dir=log_dir), # Log sparsity to TensorBoard\n",
    "        EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=False), # Increase patience slightly, disable restore_best_weights for pruning\n",
    "        ModelCheckpoint(MODEL_SAVE_PATH.replace('.h5', '_pruning.h5'), monitor='val_loss', save_best_only=False, verbose=1), # Save checkpoints during pruning\n",
    "        TensorBoard(log_dir=log_dir, histogram_freq=1) # General TensorBoard logging\n",
    "    ]\n",
    "    print(f\"\\nTensorBoard logs will be saved to: {log_dir}\")\n",
    "    print(\"Run `tensorboard --logdir logs/pruning` in your terminal to view.\")\n",
    "\n",
    "\n",
    "    # 9. Train the Pruned Model\n",
    "    print(\"\\n--- Starting Pruning-Aware Training ---\")\n",
    "    history = model_for_pruning.fit(\n",
    "        X_train_final, y_train,\n",
    "        epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_val_final, y_val),\n",
    "        callbacks=callbacks, # Use the callbacks list including pruning ones\n",
    "        class_weight=class_weight_dict, verbose=1\n",
    "    )\n",
    "    print(\"--- Pruning-Aware Training Finished ---\")\n",
    "\n",
    "    # --- Strip Pruning Wrappers ---\n",
    "    print(\"\\n--- Stripping Pruning Wrappers ---\")\n",
    "    # model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "    # It's often better to load the last saved checkpoint before stripping\n",
    "    try:\n",
    "        print(\"Loading last saved pruning checkpoint...\")\n",
    "        # Construct the checkpoint name saved by ModelCheckpoint\n",
    "        # Note: Keras saves checkpoints like 'model_name.epoch-val_loss.h5' or just overwrites MODEL_SAVE_PATH\n",
    "        # We saved with a specific name _pruning.h5, assuming it might just overwrite that.\n",
    "        # If using save_best_only=False, it might just save the final epoch state.\n",
    "        # Safest is often to load the *last* saved checkpoint explicitly if needed.\n",
    "        # For simplicity here, we assume the model in memory is the one we want,\n",
    "        # or we load the explicitly named final save.\n",
    "        # Consider loading the specific checkpoint if EarlyStopping stopped training.\n",
    "        model_to_strip = tf.keras.models.load_model(MODEL_SAVE_PATH.replace('.h5', '_pruning.h5'))\n",
    "        model_for_export = tfmot.sparsity.keras.strip_pruning(model_to_strip)\n",
    "        print(\"Loaded checkpoint and stripped pruning wrappers.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load pruning checkpoint, stripping model currently in memory: {e}\")\n",
    "        model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning) # Fallback\n",
    "\n",
    "\n",
    "    print(\"Model after stripping wrappers:\")\n",
    "    model_for_export.summary() # This is a standard Keras model now, but with zeroed weights\n",
    "\n",
    "    # Save the final stripped model (optional but good practice)\n",
    "    model_for_export.save(MODEL_SAVE_PATH)\n",
    "    print(f\"Final stripped model saved to {MODEL_SAVE_PATH}\")\n",
    "    # --- End Stripping ---\n",
    "\n",
    "\n",
    "    # 10. Evaluate the Final Stripped Model\n",
    "    print(\"\\n--- Evaluating Final Stripped Model on Test Set ---\")\n",
    "    # Re-compile the stripped model before evaluation\n",
    "    model_for_export.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    loss, accuracy = model_for_export.evaluate(X_test_final, y_test, verbose=0)\n",
    "    print(f\"Test Loss (Stripped Model): {loss:.4f}\")\n",
    "    print(f\"Test Accuracy (Stripped Model): {accuracy:.4f}\")\n",
    "\n",
    "    # Classification Report & Confusion Matrix (for the stripped model)\n",
    "    y_pred_probs = model_for_export.predict(X_test_final)\n",
    "    y_pred_int = np.argmax(y_pred_probs, axis=1); y_test_int = np.argmax(y_test, axis=1)\n",
    "    print(\"\\nClassification Report (Stripped Model):\"); print(classification_report(y_test_int, y_pred_int, target_names=label_encoder.classes_))\n",
    "    print(\"\\nConfusion Matrix (Stripped Model):\"); cm = confusion_matrix(y_test_int, y_pred_int); plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "    plt.xlabel('Predicted Label'); plt.ylabel('True Label'); plt.title('Confusion Matrix (Stripped Model)'); plt.show()\n",
    "\n",
    "\n",
    "    # 11. Save Scaler and Label Encoder (No change needed)\n",
    "    with open(SCALER_SAVE_PATH, 'wb') as f: pickle.dump(scaler, f)\n",
    "    print(f\"Scaler saved to '{SCALER_SAVE_PATH}'\")\n",
    "    with open(LABEL_ENCODER_SAVE_PATH, 'wb') as f: pickle.dump(label_encoder, f)\n",
    "    print(f\"Label encoder saved to '{LABEL_ENCODER_SAVE_PATH}'\")\n",
    "\n",
    "    # Print constants for C++ (No change needed, uses same scaler)\n",
    "    print(\"\\n--- Normalization Constants for C++ ---\")\n",
    "    # ...(rest of the print statements for constants)...\n",
    "    print(f\"// Constants generated from scaler: {SCALER_SAVE_PATH}\")\n",
    "    print(\"const float FEATURE_MEANS[NUM_FEATURES] = {\")\n",
    "    print(\"   \", \", \".join([f\"{m:.8e}\" for m in scaler.mean_]), \" // ax, ay, az,... means\")\n",
    "    print(\"};\")\n",
    "    print(\"const float FEATURE_SCALES[NUM_FEATURES] = {\")\n",
    "    print(\"   \", \", \".join([f\"{s:.8e}\" for s in scaler.scale_]), \" // ax, ay, az,... scales (std dev)\")\n",
    "    print(\"};\")\n",
    "    print(\"const char* LABELS[NUM_CLASSES] = {\", \", \".join([f'\"{c}\"' for c in label_encoder.classes_]), \"};\")\n",
    "    print(f\"#define NUM_TIMESTEPS {timesteps}\"); print(f\"#define NUM_FEATURES {features}\"); print(f\"#define NUM_CLASSES {N_CLASSES}\")\n",
    "\n",
    "\n",
    "    # --- TensorFlow Lite Conversion (Convert the STRIPPED model) ---\n",
    "    print(\"\\n--- Converting Final Stripped Model to TensorFlow Lite ---\")\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export) # <<< CONVERT THE STRIPPED MODEL\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT] # Standard optimization\n",
    "\n",
    "    # Representative dataset generator (Same as before, uses 4D validation data)\n",
    "    def representative_dataset_gen():\n",
    "        num_samples = X_val_final.shape[0]\n",
    "        samples_to_use = min(100, num_samples)\n",
    "        indices = np.random.choice(num_samples, samples_to_use, replace=False)\n",
    "        for i in indices: yield [X_val_final[i:i+1].astype(np.float32)]\n",
    "\n",
    "    # converter.representative_dataset = representative_dataset_gen # Only needed for quantization\n",
    "\n",
    "    try:\n",
    "        tflite_model_pruned = converter.convert()\n",
    "        print(\"Stripped model successfully converted to TensorFlow Lite format.\")\n",
    "\n",
    "        with open(TFLITE_MODEL_PATH, 'wb') as f: f.write(tflite_model_pruned)\n",
    "        print(f\"Pruned TFLite model saved to '{TFLITE_MODEL_PATH}' ({len(tflite_model_pruned)} bytes)\")\n",
    "\n",
    "        # Convert to C array (Same as before)\n",
    "        print(f\"\\nConverting '{TFLITE_MODEL_PATH}' to '{CHEADER_FILE_PATH}' using xxd...\")\n",
    "        # ...(rest of xxd conversion code)...\n",
    "        os.system(f\"xxd -i {TFLITE_MODEL_PATH} > {CHEADER_FILE_PATH}\")\n",
    "        if os.path.exists(CHEADER_FILE_PATH):\n",
    "             print(f\"Successfully created C header file '{CHEADER_FILE_PATH}'\")\n",
    "             print(f\"IMPORTANT: Manually edit '{CHEADER_FILE_PATH}' to ensure the array\")\n",
    "             print(f\"           is named 'g_imu_model_data' and the length variable is present.\")\n",
    "        else:\n",
    "             print(f\"Error: Failed to create C header file '{CHEADER_FILE_PATH}'. Ensure 'xxd' is installed.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!! Error during TFLite conversion: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
